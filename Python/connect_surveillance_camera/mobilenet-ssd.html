<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="kinoshita hidetoshi (木下 英俊)">
  <meta name="description" content="木下英俊が自身のためにプログラムメモを残すことを目的に作成したページです。">
  <meta name="keywords" content="">
  <!-- キャッシュ無効化 -->
  <meta http-equiv="Cache-Control" content="no-cache">
	
  <!-- タイトル -->
  <title>物体検知 － MobileNet-SSD | Programing Items</title>
	
  <!-- ファビコン -->
  <link rel="shortcut icon" href="../../favicon.ico">
  
  <!-- CSS -->
  <link href="https://unpkg.com/ress/dist/ress.min.css" rel="stylesheet">
	<link rel="stylesheet" href="../../design.css" type="text/css">
  
	<!-- Start for 'google-code-prettify' -->
	<link href="../../prettify/styles/desert.css" rel="stylesheet" type="text/css">
	<script src="../../prettify/prettify.js" type="text/javascript"></script>
	<!-- End for 'google-code-prettify' -->	
	
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-V2DZQK54C2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-V2DZQK54C2');
  </script>
  <!-- Global site tag (gtag.js) - Google Analytics -->

  <style type="text/css">
  .auto-style1 {
    background-color: #505000;
  }
  .auto-style2 {
  text-decoration: underline;
}
  .auto-style3 {
  color: #FF0000;
}
  .auto-style4 {
	background-color: #FFFF00;
  }
  .auto-style5 {
	color: #FF0000;
	background-color: #FFFF00;
  }
  .auto-style6 {
	border-width: 0px;
  }
  </style>

</head>

<body onload="prettyPrint();">
	
<h1>物体検知 － MobileNet-SSD</h1>

<p>&nbsp;</p>
<div class="mokuji">
  <nav>
    <h2>目次</h2>
    <p><a href="#1._準備">1. 準備</a></p>
    <p>&nbsp;&nbsp; <a href="#1-1._Pytorch_をインストールする">1-1. PyTorch をインストールする</a></p>
    <p>&nbsp;&nbsp; <a href="#1-2._必要なライブラリをインストール">1-2. 必要なライブラリをインストールする</a></p>
    <p><a href="#2._MobileNetV1-SSD">2. MobileNetV1-SSD</a></p>
    <p>&nbsp;&nbsp; <a href="#2-1._MobileNetV1-SSD_を動かす（PC 内蔵カメラ）">2-1. MobileNetV1-SSD を動かす（PC 内蔵カメラ）</a></p>
    <p>&nbsp;&nbsp; <a href="#2-2._MobileNetV1-SSD_を動かす（i-PRO_カメラ）">2-2. MobileNetV1-SSD を動かす（i-PRO カメラ）</a></p>
    <p><a href="#3._MobileNetV2-SSD-Lite">3. MobileNetV2-SSD-Lite</a></p>
    <p>&nbsp;&nbsp; <a href="#3-1._MobileNetV2-SSD-Lite_を動かす（PC_内蔵カメラ）">3-1. MobileNetV2-SSD-Lite を動かす（PC 内蔵カメラ）</a></p>
    <p>&nbsp;&nbsp; <a href="#3-2._MobileNetV2-SSD-Lite_を動かす（i-PRO_カメラ）">3-2. MobileNetV2-SSD-Lite を動かす（i-PRO カメラ）</a></p>
    <p>&nbsp;&nbsp; <a href="#参考：_GPU動作させる場合のソースコード修正について">参考： GPU動作させる場合のソースコード修正について</a></p>
    <p><a href="#4._MobileNetV3-SSD-Lite">4. MobileNetV3-SSD-Lite</a></p>
    <p>&nbsp;&nbsp; <a href="#4-1._MobileNetV3-SSD-Lite_を動かす（JPEGファイル）">4-1. MobileNetV3-SSD-Lite を動かす（JPEGファイル）</a></p>
    <p>&nbsp;&nbsp; <a href="#4-2._MobileNetV3-SSD-Lite_を動かす（PC_内蔵カメラ）">4-2. MobileNetV3-SSD-Lite を動かす（PC 内蔵カメラ）</a></p>
    <p>&nbsp;&nbsp; <a href="#4-3._MobileNetV3-SSD-Lite_を動かす（i-PRO_内蔵カメラ）">4-3. MobileNetV3-SSD-Lite を動かす（i-PRO カメラ）</a></p>
    <p><a href="#5._学習">5. 学習</a></p>
    <p>&nbsp;&nbsp; <a href="#5-1._まずはやってみる_(open_images)">5-1. まずはやってみる (open_images)</a></p>
    <p>&nbsp;&nbsp; <a href="#5-2._独自の画像を学習してみる_(VOC)">5-2. 独自の画像を学習してみる (VOC)</a></p>
    <br>
    <p><a href="#ライセンス">ライセンス</a></p>
    <p><a href="#参考">参考</a></p>
  </nav>
</div>
<p>&nbsp;</p>
<p><strong>MobileNet-SSD</strong> は、高速に物体物体検知を行うAIモデルの一つです。高い認識性能と共に GPU 
を搭載しない組み込み機器でも動作する軽量なモデルであることに特徴があります。</p>
<p>本ページでは、<strong>MobileNet-SSD</strong> を使ってカメラ映像を画像処理する方法について記載します。</p>
<p>MobileNetSSD は V1, V2, V3 まで発表されていますので、これらを１つずつ動作させてみたいと思います。i-PRO 
カメラと接続して使用する手順についても具体的に紹介していきます。</p>
<p>&nbsp;</p>
<p>こちら、私のノートPCで CPU 動作させた例です。GPU無しの動作環境ですがこれぐらいでリアルタイム動作できています。</p>

    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv2-ssd-pccam.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video>
    
<p>&nbsp;</p>

    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv3-ssd-ipromini_1.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video>
    
<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>
<section>
  <p class="auto-style2"><strong>"i-PRO mini" 紹介： </strong></p>
  <ul>
	<li><a href="https://cwc.i-pro.com/pages/i-pro-mini-lp" target="_blank">
	i-PRO mini</a></li>
	<li>
	<a href="https://cwc.i-pro.com/collections/camera/products/wv-s7130ux" target="_blank">
	i-PRO mini 有線LANモデル WV-S7130UX</a></li>
	<li>
	<a href="https://cwc.i-pro.com/collections/camera/products/wv-s7130wux" target="_blank">
	i-PRO mini 無線LANモデル WV-S7130WUX</a></li>
	<li>
	<a href="https://japancs.i-pro.com/space/DLJP/724085590/WV-S7130UX　i-PRO+mini+有線LANモデル" target="_blank">
	WV-S7130UX　i-PRO mini 有線LANモデル - ダウンロード - i-PRO サポートポータル</a></li>
	<li>
	<a href="https://japancs.i-pro.com/space/DLJP/724086255/WV-S7130WUX　i-PRO+mini+無線LANモデル" target="_blank">
	WV-S7130WUX　i-PRO mini 無線LANモデル - ダウンロード - i-PRO サポートポータル</a></li>
  </ul>
  <p>
  <a href="images/i-PRO_mini.jpg" target="_blank">
  <img alt="i-PRO mini 画像" class="border_with_drop-shadow" src="images/i-PRO_mini.jpg" width="348"></a></p>
  <p>&nbsp;</p>
  <p class="auto-style2"><strong>"モジュールカメラ" 紹介：</strong></p>
  <ul>
	<li><a href="https://moduca.i-pro.com" target="_blank">モジュールカメラ｜ポータルサイト 
	(i-pro.com)</a></li>
	<li>
	<a href="https://moduca.i-pro.com/space/MCT/768743132/各種マニュアル" target="_blank">
	各種マニュアル - Module Camera Technical Information - モジュールカメラ｜ポータルサイト (i-pro.com)</a></li>
  </ul>
  <p>
  <a href="images/ai_starter_kit_1.png" target="_blank">
  <img alt="AIスターターキット画像（その１）" class="border_with_drop-shadow" src="images/ai_starter_kit_1.png" width="404"></a>
  <a href="images/ai_starter_kit_2.png" target="_blank">
  <img alt="AIスターターキット画像（その２）" class="border_with_drop-shadow" src="images/ai_starter_kit_2.png" width="444"></a></p>
  <p>&nbsp;</p>
  <p>カメラ初期設定についてはカメラ毎の取扱説明書をご確認ください。</p>
  <p>カメラのIPアドレスを確認・設定できる下記ツールを事前に入手しておくと便利です。</p>
  <ul>
	<li>
	<a href="https://connect.panasonic.com/jp-ja/products-services_security_support_specifications-manuals-firms-tool_2014040315191048" target="_blank">
	IP簡単設定ソフトウェア</a>&nbsp;（日本国内）</li>
	<li>
	<a href="https://bizpartner.panasonic.net/public/file/ip-setting-software" target="_blank">
	IP Setting Software</a>&nbsp;&nbsp;&nbsp;&nbsp; （グローバル）</li>
  </ul>
</section>

<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<section>
	<h2> <a name="1._準備">1. 準備</a></h2>
	<h4>[概要]</h4>
    <p>Python を事前にインストール済みであることを前提に記載します。</p>
    <p>私の評価環境は以下の通りです。</p>
  <p>&nbsp;</p>
	
	<h4>[評価環境]</h4>
	<table>
	<tbody>
	  <tr>
	    <td class="td_separate" colspan="3"></td>
	  </tr>
		
	  <tr>
	    <td>言語 :</td>
	    <td>Python,</td>
	    <td>3.10.4 </td>
	  </tr>
		
	  <tr>
	    <td class="td_separate" colspan="3"></td>
	  </tr>
		
	  <tr>
	    <td>OS :</td>
	    <td>Windows 11 home,</td>
	    <td>21H2</td>
	  </tr>
		
	  <tr>
	    <td class="td_separate" colspan="3"></td>
	  </tr>
	</tbody>
	</table>
	
	<p>&nbsp;</p>
	<p> &nbsp;</p>
    <h3> <a name="1-1._Pytorch_をインストールする">1-1. Pytorch をインストールする</a></h3>
    <p> (1)</p>
    <p> 下記URLを開きます。</p>
    <p> <a href="https://pytorch.org/get-started/locally/" target="_blank">
    https://pytorch.org/get-started/locally/</a></p>
    <p> &nbsp;</p>
    <p> (2)</p>
    <p> 下図のような画面を表示するので、使用される環境を選択します。</p>
    <p> 私は Windows 環境で多くの人が動作する例を作成したいので、Computer Platform として CPU を選択してみました。<br>
    Package はなんとなく&nbsp;Pip を選択してみます。</p>
    <p> commandは <span class="cpp-source">pip3 install torch torchvision torchaudio</span> となりました。</p>
    <p><img alt="PyTorch ホームページ画面１" class="border_with_drop-shadow" src="mobilenet-ssd/img6.jpg" width="800"></p>
    <p><a href="mobilenet-ssd/img5.jpg" target="_blank">
    <img alt="PyTorch ホームページ画面２" class="border_with_drop-shadow" src="mobilenet-ssd/img5.jpg" width="800"></a></p>
    <p> &nbsp;</p>
    <p> ちなみに "CUDA 11.3" を選択すると <span class="cpp-source">pip3 install torch 
    torchvision torchaudio --extra-index-url 
    https://download.pytorch.org/whl/cu113</span> となりました。</p>
    <p> &nbsp;</p>
    <p> (3)</p>
    <p> 表示されたコマンドをコマンドプロンプトなどのターミナルから入力することで Pytorch をインストールします。</p>
    <p> &nbsp;</p>
    <p> <a href="image_classification_vgg/img7.gif" target="_blank">
    <img alt="PyTorch インストール画面１" class="border_with_drop-shadow" src="mobilenet-ssd/img7.gif" width="800"></a></p>
    <p> &nbsp;</p>
    <p> <a href="image_classification_vgg/img9.jpg" target="_blank">
    <img alt="PyTorch インストール画面２" class="border_with_drop-shadow" src="mobilenet-ssd/img9.jpg" width="800"></a></p>
    <p> &nbsp;</p>
    <p> <a href="image_classification_vgg/imgD.gif" target="_blank">
    <img alt="PyTorch インストール画面３" class="border_with_drop-shadow" src="mobilenet-ssd/imgD.gif" width="800"></a></p>
    <p> &nbsp;</p>
    <p> これで Pytorch のインストールを完了です。</p>
    <p> &nbsp;</p>
    <p> &nbsp;</p>
    <h3> <a name="1-2._必要なライブラリをインストール">1-2. 必要なライブラリをインストール</a></h3>
    <p> opencv を使用するので、下記コマンドによりインストールします。</p>
    <p> <span class="cpp-source">pip install opencv-python </span></p>
	
</section>
	
<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<h2><a name="2._MobileNetV1-SSD">2. MobileNetV1-SSD</a></h2>
<p>2017年に MobileNet v1 が発表されました。（<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank">MobileNet 
V1 の原著論文</a>）</p>
<p>
分類・物体検出・セマンティックセグメンテーションを含む画像認識を、モバイル端末などの限られたリソース下で高精度で判別するモデルを作成することを目的として作成しています。</p>
<p>MobileNetV1 は下記２つの技術により高速化を行っています。</p>
<ol>
  <li>Depthwise Separable Convolution</li>
  <li>Pointwise convolution</li>
</ol>
<p>&nbsp;</p>
<ul>
  <li>通常の畳み込み層演算処理を空間方向とチャンネル方向の2段階に分けて行う</li>
  <li>各チャンネル毎に独立して空間方向 (Depthwise, 3x3) のみに畳み込み演算を行う (Depthwise Separable Convolution)</li>
  <li>1x1フィルターの畳み込みによりチャンネル方向 (Pointwise, 1x1) のみに畳み込み演算を行う (Pointwise convolution)</li>
  <li>以上の結果、総演算量を 1/8～1/9 に削減</li>
</ul>
<p>&nbsp;</p>
<p><img alt="MobileNetV1 アーキテクチャ構造" src="mobilenet-ssd/img28.jpg" width="800"></p>
<p>&nbsp;</p>
<p>MobileNet(V1) のアーキテクチャ構造です。</p>
<p><img alt="MobileNetV1 Body Architecture" class="border" src="mobilenet-ssd/img3A.jpg"></p>
<p>&nbsp;</p>
<p>MobileNet(V1) の物体検出性能を、COCOデータセット上で評価・比較した結果です。論文からの引用です。</p>
<p><img alt="COCO object detection results comparison" class="border" src="mobilenet-ssd/img24.jpg" width="600"></p>
<p>引用元： <a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank">MobileNet 
V1 の原著論文</a></p>
<p>&nbsp;</p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table] 用語説明</caption>
      <thead class="standard_table">
      </thead>
      <tbody>
        <tr>
          <td>mAP</td>
          <td>認識性能の平均値（mean Average Precision）</td>
        </tr>
        <tr>
          <td>Mult-Adds</td>
          <td>積和演算の回数</td>
        </tr>
        <tr>
          <td>Parameters</td>
          <td>重みの数</td>
        </tr>
      </tbody>
    </table>
    
<p>&nbsp;</p>
<p>上記結果から、MobileNet(V1) は、認識性能（mAP）を大きく低下することなく計算量（Mult-Adds）を劇的に削減できていることを読み取れます。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

<section>
	<h3> <a name="2-1._MobileNetV1-SSD_を動かす（PC 内蔵カメラ）">2-1. MobileNetV1-SSD を動かす（PC 内蔵カメラ）</a></h3>
	<h4>[概要]</h4>
    <p>MobileNetV1-SSD を PyTorch の環境で動かしてみます。</p>
    <p><a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> というこの内容そのままのものが GitHub で公開されていました。これを取得して動作させてみます。</p>
	<p>ちなみに pytorch-ssd のライセンスは "MIT License" です。</p>
    <p><a href="mobilenet-ssd/imgC.jpg" target="_blank">
    <img alt="pytorch-ssd ホームページ画面" class="border_with_drop-shadow" src="mobilenet-ssd/imgC.jpg" width="800"></a></p>
    <p>&nbsp;</p>
	<p> &nbsp;</p>
	
  <h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.4 </td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.11.0</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>21H2</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
    <p>&nbsp;</p>
    <h4>[手順]</h4>
    <p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> から git clone してソースコード一式を入手します。</p>
	<p>任意のフォルダで端末（コマンドプロンプト等）を起動して下記コマンドをインプットします。</p>
    <p>（git を既にインストール済みとして記載します。）</p>
    <pre>git clone https://github.com/qfgaohao/pytorch-ssd.git</pre>
    <p>&nbsp;</p>
	<p>2. "git clone" したフォルダへ移動します。</p>
	<pre>cd pytorch-ssd</pre>
	<p>&nbsp;</p>
    <p>3. README.md 中の "Run the live MobilenetV1 SSD demo" に記載の内容に従って下記を実行します。</p>
    <p>最初の２行は学習済みデータとラベルデータの取得なので、初めて実行するときのみ実施すれば良いです。</p>
    <pre>wget -P models https://storage.googleapis.com/models-hao/mobilenet-v1-ssd-mp-0_675.pth 
wget -P models https://storage.googleapis.com/models-hao/voc-model-labels.txt
python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt </pre>
    <p>&nbsp;</p>
    <p>ところで Windows に標準では wget コマンドは無いので、上記を実行してもエラーになります。Windows 用の wget を入手して使っても良いですが、代わりに 
    bitsadmin コマンドで代用するという方法もあります。</p>
    <p>&nbsp;</p>
    <p class="auto-style2"><strong>bitsadmin.exe</strong> の書き方：</p>
    
    <blockquote>
      <strong>bitsadmin.exe</strong> /transfer ＜ジョブ名＞ ＜URL＞ ＜保存先ファイル名（フルパス）＞</blockquote>
    
    <p>上記内容を bitsadmin.exe で置き換えると以下のようになるので、Windows 環境の方はこちらで実施してもOKです。<br>
    <span class="auto-style3">{作業フォルダ}</span> の部分をご自身の環境に合わせて修正して実行してください。</p>
    <pre>bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/mobilenet-v1-ssd-mp-0_675.pth c:\<span class="auto-style3">{作業フォルダ}</span>\models\mobilenet-v1-ssd-mp-0_675.pth
bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/voc-model-labels.txt c:\<span class="auto-style3">{作業フォルダ}</span>\models\voc-model-labels.txt
python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt</pre>
    <p>&nbsp;</p>
	<p>学習済みモデルは VOC dataset を使って学習しているので、このプログラムは下記 20クラス を検知します。</p>
	<pre style="width: 200px">1: aeroplane
2: bicycle
3: bird
4: boat
5: bottle
6: bus
7: car
8: cat
9: chair
10: cow
11: diningtable
12: dog
13: horse
14: motorbike
15: person
16: pottedplant
17: sheep
18: sofa
19: train
20: tvmonitor</pre>
	<p>&nbsp;</p>
    <p>4. 上記手順でプログラム（"python run_ssd_live_demo.py ･･･" のところ）を実行したところ、私の環境では下図のようなエラーを表示して正常に動作しませんでした。</p>
    <p><a href="mobilenet-ssd/img9.gif" target="_blank">
    <img alt="pytorch-ssd － run_ssd_live_demo.py エラー発生時の画面" src="mobilenet-ssd/img9.gif" width="800"></a></p>
    <p>&nbsp;</p>
    <p>どうやら、box[0], box[1], box[2], box[3] 
    が浮動小数点なのですがここの引数は整数(int)である必要がある、ということがエラーの理由のようです。<br>エラーとなった 76行目と、同様に 
    79行目の２カ所を int 型へ変換するように修正します。下図だと4行目、7行目の色付きの場所へ int() を追加しました。<br>
    これでエラーを解決してプログラムを実行できるようになりました。</p>
    <pre class="prettyprint linenums:73 lang-py">
        for i in range(boxes.size(0)):
        box = boxes[i, :]
        label = f"{class_names[labels[i]]}: {probs[i]:.2f}"
        cv2.rectangle(orig_image, (<span class="auto-style1">int(</span>box[0]<span class="auto-style1">)</span>, <span class="auto-style1">int(</span>box[1]<span class="auto-style1">)</span>), (<span class="auto-style1">int(</span>box[2]<span class="auto-style1">)</span>, <span class="auto-style1">int(</span>box[3]<span class="auto-style1">)</span>), (255, 255, 0), 4)

        cv2.putText(orig_image, label,
                    (<span class="auto-style1">int(</span>box[0]<span class="auto-style1">)</span>+20, <span class="auto-style1">int(</span>box[1]<span class="auto-style1">)</span>+40),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1,  # font scale
                    (255, 0, 255),
                    2)  # line type</pre>
    <p>&nbsp;</p>
	<p>別サイトの記事記載[9] によると、OpenCVバージョンアップに伴う影響だそうです。元々はこのようなエラーは出なかったのでしょう。</p>
	<p>修正を <a href="https://github.com/qfgaohao/pytorch-ssd/pull/178" target="_blank">Pull Request #178</a> しておきましたが、ポスト後に確認したら私を含めて３件の同件修正が Pull Request 
    されていました。こちらのリポジトリはメンテを終了しているかもしれません。</p>
	<p>&nbsp;</p>
    
    <div class="status_ok" style="width: 700px">
      <div></div>
      <div>
        <p>(2022-08-03 追記)</p>
		<p>私から出した Pull Request (#178) が先ほどマージされました。</p>
		<p>このためマージ後のソースを取得（git clone）した方はこちら 4 に記載の修正は不要です。</p>
        <p>
		<a href="https://github.com/qfgaohao/pytorch-ssd/pull/178" target="_blank">Bug fix. 
		by kinoshita-hidetoshi · Pull Request #178 · qfgaohao/pytorch-ssd · 
		GitHub</a></p>
      </div>
    </div>

      <p>&nbsp;</p>
	  <p>&nbsp;</p>
    <p>&nbsp;</p>
    <p>5. 修正したプログラムを動かした様子を以下に示します。</p>
    <p>カメラはPC内蔵のカメラです。PyTorch は前述の通り CPU 版ですがかなり快適に動作できています。</p>
    <p>&nbsp;</p>
    
  	<p>[動画] プログラムを動作させた様子（PC 内蔵カメラ）</p>
    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv1-ssd-pccam.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video> <br>
    
	<p>&nbsp;</p>
	<p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>プログラム "run_ssd_live_demo.py" 中で PC内蔵カメラ からのキャプチャーを行っている部分は下記の箇所です。</p>
		    <p><span class="cpp-source">cap = cv2.VideoCapture(0)   # capture from camera</span></p>
		  </div>
    </div>

    <p>&nbsp;</p>
	<p>&nbsp;</p>
  <p>&nbsp;</p>
    
</section>
	
<p>&nbsp;</p>

<section>
	<h3><a name="2-2._MobileNetV1-SSD_を動かす（i-PRO_カメラ）">2-2. MobileNetV1-SSD を動かす（i-PRO カメラ）</a></h3>
	<h4>[概要]</h4>
    <p>MobileNetV1-SSD を PyTorch の環境で動かしてみます。</p>
	<p>本章では i-PRO カメラとPCを LAN 接続してリアルタイムで物体検知してみます。前章と同様に <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> を使って行います。</p>
    <p>&nbsp;</p>
	<p> &nbsp;</p>
	
  <h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.4 </td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.11.0</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>21H2</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
    <p>&nbsp;</p>
    <h4>[手順]</h4>
    <p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> から git clone してソースコード一式を入手します。（前章と同じです。実施済みなら不要です。）</p>
	<p>任意のフォルダで端末（コマンドプロンプト等）を起動して下記コマンドをインプットします。</p>
    <pre>git clone https://github.com/qfgaohao/pytorch-ssd.git</pre>
    <p>&nbsp;</p>
	<p>2. "git clone" したフォルダへ移動します。</p>
	<pre>cd pytorch-ssd</pre>
	<p>&nbsp;</p>
    <p>3. README.md 中の "Run the live MobilenetV1 SSD demo" に記載の内容に従って下記を実行します。（前章と同じです。実施済みなら不要です。）</p>
    <pre>wget -P models https://storage.googleapis.com/models-hao/mobilenet-v1-ssd-mp-0_675.pth 
wget -P models https://storage.googleapis.com/models-hao/voc-model-labels.txt
</pre>
    <p>&nbsp;</p>
    <p>Windows 環境で bitsadmin.exe を使って実施する場合は下記を実施します。<br>
    <span class="auto-style3">{作業フォルダ}</span> の部分をご自身の環境に合わせて修正して実行してください。</p>
    <pre>bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/mobilenet-v1-ssd-mp-0_675.pth c:\<span class="auto-style3">{作業フォルダ}</span>\models\mobilenet-v1-ssd-mp-0_675.pth
bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/voc-model-labels.txt c:\<span class="auto-style3">{作業フォルダ}</span>\models\voc-model-labels.txt
</pre>
    <p>&nbsp;</p>
	<p>4. 下記コマンドを入力してプログラムを起動します。ここでは４番目の引数として RTSP 表記で i-PRO カメラの接続を記載します。</p>
	<p><span class="auto-style3">{user-id}</span>, <span class="auto-style3">
	{password}</span>, <span class="auto-style3">{ip-address}</span> 
	の部分をご自身が使われる i-PRO カメラの設定に合わせて修正して実行してください。</p>
	<p>その他 RTSP に関しては記事「<a href="connect_with_rtsp.html">RTSP で画像を取得する</a>」を参照ください。</p>
    <pre>python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt <span class="auto-style4">rtsp://</span><span class="auto-style5">{user-id}</span>:<span class="auto-style5">{password}</span>@<span class="auto-style5">{ip-address}</span><span class="auto-style4">/MediaInput/stream_1</span></pre>
    <p>(例) <span class="cpp-source">python run_ssd_live_demo.py mb1-ssd 
	models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt 
	rtsp://admin:Admin12345@192.168.0.10/MediaInput/stream_1</span></p>
	  <p>&nbsp;</p>
	<p>5. プログラムを動かした様子を以下に示します。</p>
	<p>PyTorch は前述の通り CPU 版ですがかなり快適に動作できています。</p>
	<p>i-PRO カメラの設定を 10fps にしています。私のノートPC環境では 30fps 
	動作させると映像が少しずつ遅延していきました。AI処理なしに映像表示させると 30fps 
	表示できているので、AI処理に伴うCPU負荷に原因するものと分析します。GPU無しの環境、CPU 版での動作で Full-HD 画像を 10fps 
	でAI処理できているのですから、私個人の見解ですが、 MobileNet は十分に軽量で高性能な AI だと考えます。</p>
    <p>&nbsp;</p>
    
  	<p>[動画] プログラムを動作させた様子 １（i-PRO カメラ）</p>
    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv1-ssd-ipromini_1.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video>
    
  	<p>&nbsp;</p>
	<p>[動画] プログラムを動作させた様子 ２ － 「<a href="https://pixabay.com/ja/videos/人-商業-店-忙しい-モール-6387/" target="_blank">人 
  商業 店 - Free video on Pixabay</a>」の例（i-PRO カメラ）</p>
  <p>入力画像として、 <a href="https://pixabay.com/" target="_blank">
  https://pixabay.com</a> から取得した下記動画をテストに使用させていただきました。商用利用無料、帰属表示必要なし、のコンテンツです。</p>
  <p>PC上で再生表示する動画を i-PRO カメラで接写しているため、画質が荒いこと、認識精度が微妙なこと、はご容赦ください。</p>

    <p>
	こちらの例の場合、人が一定より小さいと認識できないようでした。カメラをディスプレに近づけて人のサイズを大きくすることで、AIが人を認識できるようになりました。</p>
	<p>別ページ紹介の「<a href="connect_to_wv-xae200w.html">機能拡張ソフトウェア(WV-XAE200W)</a>」でも同じ映像を使って評価していますが、こちらの評価では同じ映像で小さい人を認識できています。この比較からも専用商品である 
	WV-XAE200WUX の画像認識性能の高さを再確認させていただきました。</p>
    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv1-ssd-ipromini_2.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video>

    <p>&nbsp;</p>
	<p>&nbsp;</p>
	<p>以上です。というわけで、プログラムを１行も書くことなく i-PRO カメラとPCを接続して MobileNetV1-SSD 
	を実行することができました。</p>
	<p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>プログラム "run_ssd_live_demo.py" 中で i-PRO カメラからのキャプチャーを行っている部分は下記の箇所です。</p>
		    <p><span class="cpp-source">cap = cv2.VideoCapture(sys.argv[4])  # capture from file</span></p>
		  </div>
    </div>

    <p>&nbsp;</p>
    
</section>
	
<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<h2><a name="3._MobileNetV2-SSD-Lite">3. MobileNetV2-SSD-Lite</a></h2>
<p>2018年に MobileNet v1 の後継モデルとして v2 が発表されました。（<a href="https://arxiv.org/pdf/1801.04381v3.pdf" target="_blank">MobileNet V2 
の原著論文</a>）</p>
<p>MobileNet v1 
の構想およびモデルを基礎にしながら、モジュールを大幅に改良したものです。</p>
<p>MobileNet v1 
同様、分類・物体検出・セマンティックセグメンテーションを含む画像認識を、モバイル端末などの限られたリソース下で高精度で判別するモデルを作成することを目的として作成しています。</p>
<p>&nbsp;</p>
<p>MobileNetV2 で利用されているモジュールは以下の３つです。本ページでは詳細を割愛します。詳しくは原著論文などを参照ください。</p>
<ol>
  <li><strong>Depthwise Separable Convolutions</strong>（深さ方向分離可能畳み込み）<br>MobileNetV2 でも“Depthwise 
  Separable Convolutions” を利用することで、計算量を減らしています。 </li>
  <li><strong>Linear Bottlenecks</strong>（線形ボトルネック）<br>
  活性関数に非線形性をもつ層（ReLU層やSoftmax層、tanh層など）を利用すると、非常に多くの情報が失われることがあきらかになっています。そのため、ReLUにつながる中間部分のチャンネルを拡大する（＝次元を増やす）ことによって、本来つぶれてしまう情報を他のチャンネルに持たせることができ、情報の喪失を防げるという仮説にもとづき 
  MobileNetV2 では “Bottolneck Convolution” 
  が実装されている。なお、次元を大きくしすぎてもうまく機能しないことが知られており、論文では拡大率(t)を 6 に設定しています。 </li>
  <li><strong>Inverted residuals</strong>（反転残差）<br>ReLUの表現力の問題に関連してInverted residulals 
  blockを考案しています。このblockを基本的に MobileNetV1 の単純な Depthwise Separable 
  Convolution と置き換えます。<br><br>① Inverted residuals block は3つの convolution 
  から構成されています。１つ目は、1×1Conv です。そしてこのConvは t倍(tはthe expantion ratio展開率) 
  に出力チャンネルを写像する役割を持っています。２つ目は、Depthwise Separable Convolutions 
  です。３つ目は、１つ目と同様に1×1convです。こちらは出力チャンネルを入力時のチャンネル次元数に戻すようなConvolutionになっています。 <br>
  <br>② 従来の残差ネットワーク（Residual 
  Network:ResNet）と同様に、ショートカットを使用することで、より高速なトレーニングと精度の向上を可能にしています。 </li>
</ol>
<p>&nbsp;</p>
<p>表現が異なるので単純比較できませんが、V1およびV2のアーキテクチャ構造比較です。類似する場所を同じ色でマーキングしてみました。</p>
<p>V1で「"Conv dw/s1" + "Conv/s1」のセットと 
V2の1階層が対応する、という感じで読むと概ね同じような構造であることを読み取れると思います。</p>
<p><img alt="V2, V1 アーキテクチャ構造" class="border" src="mobilenet-ssd/img2D.jpg"></p>
<p>&nbsp;</p>
<ul>
  <li>上記アーキテクチャ構造について同一インプットサイズのフィルターを V1, V2 
  で比較すると、V2のチャネル数がとても小さくなっていることがわかります。</li>
  <li>例えば、Input サイズ 56x56 の部分を比較してみましょう。（水色の部分）<br>V2 では 56×56×24 
  を3回(n)実施していますが、V1 では 56x56x128 を2回実施しています。少々乱暴ですが、チャネル数で 256(v1) -&gt; 72(v2) 
  と大幅に削減しています。<br>このチャネル数の削減が計算量の削減に大きく貢献しています。</li>
</ul>
<p>&nbsp;</p>
<p>MobileNetV2とMobileNetV1の物体検出性能を、COCOデータセット上で評価・比較した結果です。論文からの引用です。</p>
<p><img alt="MobileNetV2 性能比較" src="mobilenet-ssd/img2F.jpg" class="border"></p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table] 用語説明</caption>
      <thead class="standard_table">
      </thead>
      <tbody>
        <tr>
          <td>mAP</td>
          <td>認識性能の平均値（mean Average Precision）</td>
        </tr>
        <tr>
          <td>Mult-Adds</td>
          <td>積和演算の回数</td>
        </tr>
        <tr>
          <td>Parameters</td>
          <td>重みの数</td>
        </tr>
      </tbody>
    </table>
    
<p>&nbsp;</p>
<p>以上の結果から、MobileNetV2 SSDLite は最も効率的なモデルであるだけでなく、COCOデータセット上で YOLOv2 
を上回る20倍の効率性と10倍の小型化を実現していることを読み取れます。</p>
<p>V1、V2 の比較についても、V2は認識精度（mAP）を低下することなく計算量（MAdd）を約4割削減できていることがわかります。</p>
<p>&nbsp;</p>
<p>引用元： <a href="https://arxiv.org/pdf/1801.04381v3.pdf" target="_blank">MobileNet V2 
の原著論文</a></p>

<p>&nbsp;</p>
<p>&nbsp;</p>

<section>
	<h2> <a name="3-1._MobileNetV2-SSD-Lite_を動かす（PC_内蔵カメラ）">3-1. MobileNetV2-SSD-Lite を動かす（PC 内蔵カメラ）</a></h2>
	<h4>[概要]</h4>
    <p>MobileNetV2-SSD を PyTorch の環境で動かしてみます。</p>
    <p>MobileNetV1-SSD 同様に <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> に含まれていますので、これを取得して動作させてみます。</p>
    <p><a href="mobilenet-ssd/imgC.jpg" target="_blank">
    <img alt="pytorch-ssd ホームページ画面" class="border_with_drop-shadow" src="mobilenet-ssd/imgC.jpg" width="800"></a></p>
    <p>&nbsp;</p>
	<p> &nbsp;</p>
	
  <h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.4 </td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.11.0</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>21H2</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
    <p>&nbsp;</p>
    <h4>[手順]</h4>
    <p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> から git clone してソースコード一式を入手します。（前章と同じです。実施済みなら不要です。）</p>
	<p>任意のフォルダで端末（コマンドプロンプト等）を起動して下記コマンドをインプットします。</p>
    <pre>git clone https://github.com/qfgaohao/pytorch-ssd.git</pre>
    <p>&nbsp;</p>
	<p>2. "git clone" したフォルダへ移動します。</p>
	<pre>cd pytorch-ssd</pre>
	<p>&nbsp;</p>
    <p>3. README.md 中の "Run the live MobileNetV2 SSD Lite demo" に記載の内容に従って下記を実行します。</p>
    <p>最初の２行は学習済みデータとラベルデータの取得なので、初めて実行するときのみ実施すれば良いです。また２行目の 
	"voc-model-labels.txt" は MobileNetV1-SSD と同一のファイルです。</p>
    <pre>
wget -P models https://storage.googleapis.com/models-hao/mb2-ssd-lite-mp-0_686.pth
wget -P models https://storage.googleapis.com/models-hao/voc-model-labels.txt
python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt</pre>
    <p>&nbsp;</p>
    
    <p>上記内容を bitsadmin.exe で置き換えると以下のようになるので、Windows 環境の方はこちらで実施してもOKです。<br>
    <span class="auto-style3">{作業フォルダ}</span> の部分をご自身の環境に合わせて修正して実行してください。</p>
    <pre>bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/mb2-ssd-lite-mp-0_686.pth c:\<span class="auto-style3">{作業フォルダ}</span>\models\mb2-ssd-lite-mp-0_686.pth
bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/voc-model-labels.txt c:\<span class="auto-style3">{作業フォルダ}</span>\models\voc-model-labels.txt
python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt</pre>
    <p>&nbsp;</p>
    <p>&nbsp;</p>
    <p>4. プログラムを動かした様子を以下に示します。</p>
    <p>カメラはPC内蔵のカメラです。PyTorch は前述の通り CPU 版ですがかなり快適に動作できています。</p>
    <p>&nbsp;</p>
    
  	<p>[動画] プログラムを動作させた様子（PC 内蔵カメラ）</p>
    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv2-ssd-pccam.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video> <br>
    
	<p>&nbsp;</p>
	<p>ちゃんと MobiletNetV2-ssd-lite で動作できていると思いますが、こちら映像を見るだけでは V1, V2 の違いはわからないですね。</p>
	<p>私が触ってみた印象ですが、認識精度が若干向上して CPU 負荷も若干軽くなったという気がします。</p>
	<p>&nbsp;</p>
  
  <p>&nbsp;</p>
    
</section>
	
<p>&nbsp;</p>

<section>
	<h3><a name="3-2._MobileNetV2-SSD-Lite_を動かす（i-PRO_カメラ）">3-2. MobileNetV2-SSD-Lite を動かす（i-PRO カメラ）</a></h3>
	<h4>[概要]</h4>
    <p>MobileNetV2-SSD-lite を PyTorch の環境で動かしてみます。</p>
	<p>本章では i-PRO カメラとPCを LAN 接続してリアルタイムで物体検知してみます。前章と同様に <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> を使って行います。</p>
    <p>&nbsp;</p>
	<p> &nbsp;</p>
	
  <h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.4 </td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.11.0</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>21H2</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
    <p>&nbsp;</p>
    <h4>[手順]</h4>
    <p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> から git clone してソースコード一式を入手します。（前章と同じです。実施済みなら不要です。）</p>
	<p>任意のフォルダで端末（コマンドプロンプト等）を起動して下記コマンドをインプットします。</p>
    <pre>git clone https://github.com/qfgaohao/pytorch-ssd.git</pre>
    <p>&nbsp;</p>
	<p>2. "git clone" したフォルダへ移動します。</p>
	<pre>cd pytorch-ssd</pre>
	<p>&nbsp;</p>
    
    <p>3. README.md 中の "Run the live MobilenetV1 SSD demo" に記載の内容に従って下記を実行します。（前章と同じです。実施済みなら不要です。）</p>
    <pre>wget -P models https://storage.googleapis.com/models-hao/mb2-ssd-lite-mp-0_686.pth
wget -P models https://storage.googleapis.com/models-hao/voc-model-labels.txt</pre>
    <p>&nbsp;</p>
    <p>Windows 環境で bitsadmin.exe を使って実施する場合は下記を実施します。<br>
    <span class="auto-style3">{作業フォルダ}</span> の部分をご自身の環境に合わせて修正して実行してください。</p>
    <pre>bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/mb2-ssd-lite-mp-0_686.pth c:\<span class="auto-style3">{作業フォルダ}</span>\models\mb2-ssd-lite-mp-0_686.pth
bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/voc-model-labels.txt c:\<span class="auto-style3">{作業フォルダ}</span>\models\voc-model-labels.txt</pre>
    <p>&nbsp;</p>
    
	<p>4. 下記コマンドを入力してプログラムを起動します。ここでは４番目の引数として RTSP 表記で i-PRO カメラの接続を記載します。</p>
	<p><span class="auto-style3">{user-id}</span>, <span class="auto-style3">
	{password}</span>, <span class="auto-style3">{ip-address}</span> 
	の部分をご自身が使われる i-PRO カメラの設定に合わせて修正して実行してください。</p>
	<p>その他 RTSP に関しては記事「<a href="connect_with_rtsp.html">RTSP で画像を取得する</a>」を参照ください。</p>
    <pre>python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt <span class="auto-style4">rtsp://</span><span class="auto-style5">{user-id}</span>:<span class="auto-style5">{password}</span>@<span class="auto-style5">{ip-address}</span><span class="auto-style4">/MediaInput/stream_1</span></pre>
    <p>(例) <span class="cpp-source">python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt 
	rtsp://admin:Admin12345@192.168.0.10/MediaInput/stream_1</span></p>
	  <p>&nbsp;</p>
    <p>&nbsp;</p>
    
	<p>5. プログラムを動かした様子を以下に示します。</p>
	<p>PyTorch は前述の通り CPU 版ですがかなり快適に動作できています。</p>
	<p>i-PRO カメラの設定を 10fps にしています。私のノートPC環境では 30fps 
	動作させると映像が少しずつ遅延していきました。AI処理なしに映像表示させると 30fps 
	表示できているので、AI処理に伴うCPU負荷に原因するものと分析します。GPU無しの環境、CPU 版での動作で Full-HD 画像を 10fps 
	でAI処理できているのですから、私個人の見解ですが、 MobileNet は十分に軽量で高性能な AI だと考えます。</p>
    <p>&nbsp;</p>
    
  	<p>[動画] プログラムを動作させた様子（i-PRO カメラ）</p>
    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv2-ssd-ipromini_1.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video>
    
  	<p>&nbsp;</p>
	<p>ちゃんと MobiletNetV2-ssd-lite で動作できていると思いますが、こちら映像を見るだけでは V1, V2 の違いはわからないですね。</p>
	<p>私が触ってみた印象ですが、認識精度が若干向上して CPU 負荷も若干軽くなったという気がします。</p>
	<p>&nbsp;</p>
	<p>&nbsp;</p>
	<p>以上です。というわけで、プログラムを１行も書くことなく i-PRO カメラとPCを接続して MobileNetV2-SSD 
	も実行することができました。</p>
	<p>&nbsp;</p>
</section>

<p>&nbsp;</p>
	
<section>
	<h3><a name="参考：_GPU動作させる場合のソースコード修正について">参考： GPU動作させる場合のソースコード修正について</a></h3>
	<p>本ページの紹介は Pytorch 動作を "CPU" 
	として説明していますが、GPU（CUDA）で動作させた場合に必要となるソースコード修正について参考記載します。</p>
	<p>MobileNetV1 については CUDA 環境で問題なく動作したのですが、MobileNetV2 について私の環境では下記修正を行う必要がありました。</p>
	<p>対象ファイルは "vision/ssd/mobilenet_v2_ssd_lite.py" です。</p>
	<p>&nbsp;</p>
	<p>
	<a href="mobilenet-ssd/imgC1.jpg" target="_blank">
	<img alt="GPU動作させる場合のソースコード修正について" src="mobilenet-ssd/imgC1.jpg" class="border" width="800"></a></p>
	<p>&nbsp;</p>
  
</section>
	
<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<h2><a name="4._MobileNetV3-SSD-Lite">4. MobileNetV3-SSD-Lite</a></h2>
<p>2019年に MobileNet v2 の後継モデルとして v3 が発表されました。（<a href="https://arxiv.org/pdf/1905.02244.pdf" target="_blank">MobileNet V3 
の原著論文</a>）</p>
<p>MobileNetV3 の要点は下記２つの技術です。本ページでは詳細を割愛します。詳しくは原著論文などを参照ください。</p>
<ol>
  <li>Squeeze-and-Excite</li>
  <li>h-swish</li>
</ol>
<p>&nbsp;</p>
<p>MobileNetV3 は Large と Small の２つのモデルを報告しています。アーキテクチャ構造を以下に示します。</p>
<p><img alt="MobileNetV3 アーキテクチャ構造" class="border" src="mobilenet-ssd/img37.jpg"></p>
<p>&nbsp;</p>
<p>MobileNetV3 の物体検出性能を、COCOデータセット上で評価・比較した結果です。論文からの引用です。</p>
<p><img alt="MobileNetV3 性能比較" src="mobilenet-ssd/img35.jpg"></p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table] 用語説明</caption>
      <thead class="standard_table">
      </thead>
      <tbody>
        <tr>
          <td>mAP</td>
          <td>認識性能の平均値（mean Average Precision）</td>
        </tr>
        <tr>
          <td>Mult-Adds</td>
          <td>積和演算の回数</td>
        </tr>
        <tr>
          <td>Parameters</td>
          <td>重みの数</td>
        </tr>
      </tbody>
    </table>
    
<p>&nbsp;</p>
<p>上記結果から V3 (Large) は、V1,V2 と同等の認識性能（mAP）を保持しながら、V1 の約半分、V2 
の約７割の演算量（MAdds）という軽量なモデルを実現したと読み取れます。</p>
<p>V3-Small は Large のさらに半分の演算量を実現してますが認識性能は相応に低下するようです。 </p>
<p>&nbsp;</p>
<p>&nbsp;</p>

<section>
	<h2> <a name="4-1._MobileNetV3-SSD-Lite_を動かす（JPEGファイル）">4-1. MobileNetV3-SSD-Lite を動かす（JPEGファイル）</a></h2>
	<h4>[概要]</h4>

	<p>MobileNetV3-SSD-Lite は PyTorch に組み込まれていました。"ssdlite320_mobilenet_v3_large" 
	というモジュールです。同様に学習済みモデルも Pytorch 環境のみで取得可能です。</p>
    <p>本章では Pytorch および "ssdlite320_mobilenet_v3_large" を使って 
	MobileNetV3-SSD-Lite を動かしてみたいと思います。</p>
	<p>（"ssdlite320_mobilenet_v3_large" のインプットサイズはファイル名記載の通り 320x320 
	にアレンジされているように見えます。詳細は割愛します。）</p>
	<p>&nbsp;</p>
	<p>PyTorch の資料を参考に、JPEGファイルを読み込んで物体認識する Python プログラムを作成してみます。</p>
	<p>参考にする元資料は 
	<a href="https://pytorch.org/vision/main/models.html#object-detection" target="_blank">
	こちら</a> (Object Detection) です。<br>この元資料の "fasterrcnn_resnet50_fpn_v2" を "<strong>ssdlite320_mobilenet_v3_large</strong>" 
	へ変更して、課題ある部分をさらに追加修正する、という感じでやってみます。</p>
	<p>学習済みモデルは COCO dataset を使って学習しているので、このプログラムは下記 91クラス を検知します。</p>
	<p>このプログラムを初めて実行するとき、このプログラムは学習済みモデルのダウンロードを行うため、プログラム起動に多くの時間を待つ必要があるでしょう。 </p>
	<p>&nbsp;</p>
	<pre style="width: 200px">1: person
2: bicycle
3: car
4: motorcycle
5: airplane
6: bus
7: train
8: truck
9: boat
10: traffic light
11: fire hydrant
12: street sign*
13: stop sign
14: parking meter
15: bench
16: bird
17: cat
18: dog
19: horse
20: sheep
21: cow
22: elephant
23: bear
24: zebra
25: giraffe
26: hat
27: backpack
28: umbrella
29: shoe
30: eye glasses
31: handbag
32: tie
33: suitcase
34: frisbee
35: skis
36: snowboard
37: sports ball
38: kite
39: baseball bat
40: baseball glove
41: skateboard
42: surfboard
43: tennis racket
44: bottle
45: plate
46: wine glass
47: cup
48: fork
49: knife
50: spoon
51: bowl
52: banana
53: apple
54: sandwich
55: orange
56: broccoli
57: carrot
58: hot dog
59: pizza
60: donut
61: cake
62: chair
63: couch
64: potted plant
65: bed
66: mirror
67: dining table
68: window
69: desk
70: toilet
71: door
72: tv
73: laptop
74: mouse
75: remote
76: keyboard
77: cell phone
78: microwave
79: oven
80: toaster
81: sink
82: refrigerator
83: blender
84: book
85: clock
86: vase
87: scissors
88: teddy bear
89: hair drier
90: toothbrush
91: hair brush</pre>
	<p>&nbsp;</p>
	<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
	&nbsp;</p>
	<h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.4 </td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.12.1</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>21H2</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
  
  <p>&nbsp;</p>
	<p>[run_mobilenetv3-ssdlite_jpeg_demo.py]</p>
	<pre class="prettyprint linenums lang-py">'''
[Abstract]
    Create a python program that detects objects in JPEG images using the trained AI model of "MobileNetV3 SSD-Lite".

    "MobileNetV3 SSD-Lite" の学習済みAIモデルを使用して、JPEG画像を対象に物体検知する Python プログラムを作成します。

[Details]
    This program uses pytorch "ssdlite320_mobilenet_v3_large" and pre-trained model.
    The program detects 91 classes because the trained model is trained using the COCO dataset.
    When you run this program for the first time, you will have to wait a lot of time to start the program, as it downloads the trained model.

    このプログラムは pytorch の "ssdlite320_mobilenet_v3_large" および 学習済みモデルを使用します。
    学習済みモデルは COCO dataset を使って学習しているので、このプログラムは91クラスを検知します。
    このプログラムを初めて実行するとき、このプログラムは学習済みモデルのダウンロードを行うため、あなたはプログラム起動に多くの時間を待つ必要があるでしょう。

[Library install]
    cv2:        pip install opencv-python
    pytorch:    pip install torch torchvision torchaudio

[Reference URL]
    https://pytorch.org/vision/main/models.html#object-detection
    https://pytorch.org/vision/0.11/auto_examples/plot_visualization_utils.html#visualizing-bounding-boxes

[Image used]
    dog1.jpg : https://pixabay.com/ja/photos/%e3%83%9e%e3%83%ab%e3%82%bf%e8%aa%9e-%e7%8a%ac-%e5%ad%90%e7%8a%ac-%e5%b0%8f%e5%9e%8b%e7%8a%ac-1123016/
'''

from pathlib import Path
from torchvision.io.image import read_image
from torchvision.models.detection import <span class="auto-style1">ssdlite320_mobilenet_v3_large</span>, <span class="auto-style1">SSDLite320_MobileNet_V3_Large_Weights</span>
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image

img = read_image(str(Path('assets') / 'dog1.jpg'))

# Step 1: Initialize model with the best available weights
weights = <span class="auto-style1">SSDLite320_MobileNet_V3_Large_Weights</span>.DEFAULT
model = <span class="auto-style1">ssdlite320_mobilenet_v3_large</span>(weights=weights)
model = model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = [preprocess(img)]

# Step 4: Use the model and visualize the prediction
prediction = model(batch)[0]
score_threshold = 0.5
labels = [weights.meta["categories"][class_index] + f": {score_int:.2f}" for class_index, score_int in zip(prediction["labels"], prediction["scores"]) if score_int > score_threshold]
boxes = prediction["boxes"][prediction["scores"] > score_threshold]
box = draw_bounding_boxes(img, boxes=boxes,
                          labels=labels,
                          colors="red",
                          width=4,
                          font='arial.ttf', font_size=30)
im = to_pil_image(box.detach())
im.show()</pre>
	<p>&nbsp;</p>
	<p>
	たったこれだけのソースコードで物体検知を実現できます。学習済みモデルのダウンロードを含みますので、必ずインターネットへ接続できる環境で実行してください。</p>
	<p>&nbsp;</p>
	<p>動作結果はこちらです。</p>
	<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
	評価画像として、<span>&nbsp;</span><a href="https://pixabay.com" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 0px; background-color: transparent;" target="_blank">https://pixabay.com</a><span>&nbsp;提供の</span>画像を使用させていただきました。商用利用無料、帰属表示必要なし、の画像です。<br>
	<a href="https://pixabay.com/ja/photos/マルタ語-犬-子犬-小型犬-1123016/" target="_blank">https://pixabay.com/ja/photos/%e3%83%9e%e3%83%ab%e3%82%bf%e8%aa%9e-%e7%8a%ac-%e5%ad%90%e7%8a%ac-%e5%b0%8f%e5%9e%8b%e7%8a%ac-1123016/</a></p>
	<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
	上記プログラムを実行する際は、ご自身で画像を準備またはダウンロードして事前に "assets" フォルダに "dog1.jpg" 
	の名称で保存してください。</p>
	<p><img alt="MobileNetV3 静止画による物体検知 実施例" src="mobilenet-ssd/img8.jpg" width="800"></p>
    
</section>
	
<p>&nbsp;</p>
<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
学習済みモデルは下記に保存されます。'~' はログインしているユーザーのホームディレクトリを意味します。</p>
<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
&nbsp;</p>
<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
<a href="https://pytorch.org/docs/stable/hub.html#where-are-my-downloaded-models-saved" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 0px; background-color: transparent;" target="_blank">
Where are my downloaded models saved?</a></p>
<blockquote style="background-repeat: no-repeat; box-sizing: inherit; margin: 2em 0px; padding: 1em; background-color: rgb(250, 250, 250); color: rgb(85, 85, 85); border-left: 8px solid rgb(224, 224, 224); font-size: 13.3px; display: block; margin-block: 1em; font-family: sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
  <p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif;">
  The locations are used in the order of</p>
  <ul style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 5px 0px 5px 20px; font-family: sans-serif; list-style-position: outside; list-style-type: disc;">
	<li style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 0px;">
	Calling<span>&nbsp;</span><span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">hub.set_dir(&lt;PATH_TO_HUB_DIR&gt;)</span></li>
	<li style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 0px;">
	<span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">
	$TORCH_HOME/hub</span>, if environment variable<span>&nbsp;</span><span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">TORCH_HOME</span><span>&nbsp;</span>is 
	set.</li>
	<li style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 0px;">
	<span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">
	$XDG_CACHE_HOME/torch/hub</span>, if environment variable<span>&nbsp;</span><span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">XDG_CACHE_HOME</span><span>&nbsp;</span>is 
	set.</li>
	<li style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 0px;">
	<span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">
	~/.cache/torch/hub</span></li>
  </ul>
</blockquote>
<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
私の場合は下記に ssdlite320_mobilenet_v3_large_coco-a79551df.pth 
というファイルを保存していました。約13MBのファイルサイズでした。</p>
<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
"~\.cache\torch\hub\checkpoints\ssdlite320_mobilenet_v3_large_coco-a79551df.pth"</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

<section>
	<h2> <a name="4-2._MobileNetV3-SSD-Lite_を動かす（PC_内蔵カメラ）">4-2. MobileNetV3-SSD-Lite を動かす（PC 内蔵カメラ）</a></h2>
	<h4>[概要]</h4>
	<p>4-1 で作成した JPEG 
	画像を対象に物体検知するプログラムを元に、ここではPC内蔵カメラの映像をライブで物体検知するプログラムを作成してみます。</p>
	<p>PC内蔵カメラからの映像取得は OpenCV を使って行います。</p>
	<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
	&nbsp;</p>
	<h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.4 </td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.12.1</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>21H2</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
    
    <div class="status_ok">
      <div></div>
      <div>
        <p><strong>ポイント</strong></p>
        <p>OpenCV 形式で取得した画像を PyTorch が扱える形式へ変換する必要があります。</p>
		<ul>
		  <li>OpenCVの画像は BGR の並び順になっています。BGR を RGB へ並び替えます。</li>
		  <li>&nbsp;データの並び順を [width][height][channel] から 
		  [channel][width][height] へ変換します。</li>
		  <li>各画素のデータ形式を整数(0-255)から浮動小数点(0.0-1.0)へ変換します。</li>
		</ul>
      </div>
    </div>

    <p>&nbsp;</p>
  
	<p>[run_mobilenetv3-ssdlite_live_demo.py]</p>
	<pre class="prettyprint linenums lang-py">'''
[Abstract]
    Create a python program to detect objects in the camera live video using the trained AI model of "MobileNetV3 SSD-Lite".

    "MobileNetV3 SSD-Lite" の学習済みAIモデルを使用して、カメラライブ映像を物体検知する Python プログラムを作成してみます。

[Details]
    This program uses pytorch "ssdlite320_mobilenet_v3_large" and pre-trained model.
    The program detects 91 classes because the trained model is trained using the COCO dataset.
    When you run this program for the first time, you will have to wait a lot of time to start the program, as it downloads the trained model.

    このプログラムは pytorch の "ssdlite320_mobilenet_v3_large" および 学習済みモデルを使用します。
    学習済みモデルは COCO dataset を使って学習しているので、このプログラムは91クラスを検知します。
    このプログラムを始めて実行するとき、このプログラムは学習済みモデルのダウンロードを行うため、あなたはプログラム起動に多くの時間を待つ必要があるでしょう。

[Library install]
    cv2:        pip install opencv-python
    pytorch:    pip install torch torchvision torchaudio
'''

import cv2
import torch
from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights
from torchvision.transforms.functional import convert_image_dtype
from torchvision import transforms


# Step 1: Initialize model with the best available weights
weights = SSDLite320_MobileNet_V3_Large_Weights.DEFAULT
model = ssdlite320_mobilenet_v3_large(weights=weights)
model = model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Initialize variables.
cap = cv2.VideoCapture(0)       # Capture from camera.
#cap.set(3, 1920)               # Set video stream frame width.  Remove '#' and change the value according to your needs.
#cap.set(4, 1080)               # Set video stream frame height. Remove '#' and change the value according to your needs.
winname = "Annotated"           # Window title.

# Exception definition.
BackendError = type('BackendError', (Exception,), {})


def IsWindowVisible(winname):
    '''
    [Abstract]
        Check if the target window exists.
        対象ウィンドウが存在するかを確認する。
    [Param]
        winname :       Window title
    [Return]
        True :          exist
                        存在する
        False :         not exist
                        存在しない
    [Exception]
        BackendError :
    '''
    try:
        ret = cv2.getWindowProperty(winname, cv2.WND_PROP_VISIBLE)
        if ret == -1:
            raise BackendError('Use Qt as backend to check whether window is visible or not.')

        return bool(ret)

    except cv2.error:
        return False


while True:
    # Capture image by opencv.
    ret, orig_image = cap.read()
    if orig_image is None:
        continue

    # Convert image from BGR to RGB.
    rgb_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)

    # Convert image from numpy.ndarray to torchvision image format.
    rgb_image = rgb_image.transpose((2, 0, 1))
    rgb_image = rgb_image / 255.0
    rgb_image = torch.FloatTensor(rgb_image)

    # Step 3: Apply inference preprocessing transforms
    batch = [preprocess(rgb_image)]

    # Step 4: Use the model and visualize the prediction
    with torch.no_grad():
        prediction = model(batch)[0]

    score_threshold = 0.5
    labels = [weights.meta["categories"][class_index] + f": {score_int:.2f}" for class_index, score_int in zip(prediction["labels"], prediction["scores"]) if score_int &gt; score_threshold]
    boxes = prediction["boxes"][prediction["scores"] &gt; score_threshold]

    # Draw result.
    for box, label in zip(boxes, labels):

        cv2.rectangle(
            orig_image,                         # Image.
            (int(box[0]), int(box[1])),         # Vertex of the rectangle.
            (int(box[2]), int(box[3])),         # Vertex of the rectangle opposite to pt1.
            (255, 255, 0),                      # Color.
            4 )                                 # Line type.

        cv2.putText(
            orig_image,                         # Image.
            label,                              # Text string to drawn.
            (int(box[0])+20, int(box[1])+40),   # Bottom-left corner of the text string in the image.
            cv2.FONT_HERSHEY_SIMPLEX,           # Font face. - フォント種別
            0.8,                                # Font scale.
            (255, 0, 255),                      # Color.
            2)                                  # Line type.

    # Display video.
    cv2.imshow(winname, orig_image)

    # Press the "q" key to finish.
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break

    # Exit the program if there is no specified window.
    if not IsWindowVisible(winname):
        break

cap.release()
cv2.destroyAllWindows()
</pre>
	<p>&nbsp;</p>
    
  	<p>[動画] プログラムを動作させた様子（PC 内蔵カメラ）</p>
    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv3-ssd-pccam.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video> <br>
    
	<p>&nbsp;</p>
    
</section>
	
<p>&nbsp;</p>

<section>
	<h2> <a name="4-3._MobileNetV3-SSD-Lite_を動かす（i-PRO_内蔵カメラ）">4-3. MobileNetV3-SSD-Lite を動かす（i-PRO カメラ）</a></h2>
	<h4>[概要]</h4>
	<p>4-2 で作成したプログラムを元に i-PRO カメラと接続してカメラ映像をライブで物体検知するプログラムを作成してみます。</p>
	<p>cv2.VideoCapture の引数を "0" から RTSP表記 へ変更するだけです。</p>
	<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
	&nbsp;</p>
	<h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.4 </td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.12.1</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>21H2</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
    
    <div class="status_ok">
      <div></div>
      <div>
        <p><strong>ポイント</strong></p>
        <p>cv2.VideoCapture 関数の引数として RTSP 
		と呼ばれる表記でネットワークカメラとの接続情報を渡します。この表記方法はネットワークカメラによって異なります。下記サンプルソースコードの表記は 
		i-PRO カメラで使用されている表記です。</p>
      </div>
    </div>

    <p>&nbsp;</p>
	<p>"user-id", "password", "host" 
	の部分をご自身が使われる i-PRO カメラの設定に合わせて修正してください。</p>
	<p>その他 RTSP に関しては記事「<a href="connect_with_rtsp.html">RTSP で画像を取得する</a>」を参照ください。</p>
  
  <p>&nbsp;</p>
	<p>[run_mobilenetv3-ssdlite_live_i-pro-com_demo.py]</p>
	<pre class="prettyprint linenums lang-py">'''
[Abstract]
    Create a python program to detect objects in the camera live video using the trained AI model of "MobileNetV3 SSD-Lite".

    "MobileNetV3 SSD-Lite" の学習済みAIモデルを使用して、カメラライブ映像を物体検知する Python プログラムを作成してみます。

[Details]
    This program uses pytorch "ssdlite320_mobilenet_v3_large" and pre-trained model.
    The program detects 91 classes because the trained model is trained using the COCO dataset.
    When you run this program for the first time, you will have to wait a lot of time to start the program, as it downloads the trained model.

    このプログラムは pytorch の "ssdlite320_mobilenet_v3_large" および 学習済みモデルを使用します。
    学習済みモデルは COCO dataset を使って学習しているので、このプログラムは91クラスを検知します。
    このプログラムを始めて実行するとき、このプログラムは学習済みモデルのダウンロードを行うため、あなたはプログラム起動に多くの時間を待つ必要があるでしょう。

[Library install]
    cv2:        pip install opencv-python
    pytorch:    pip install torch torchvision torchaudio
'''

import cv2
import torch
from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights
from torchvision.transforms.functional import convert_image_dtype
from torchvision import transforms


# Initialize variables.
<span class="auto-style1">user_id     = "user-id"             # Change to match your camera setting</span>
<span class="auto-style1">user_pw     = "password"            # Change to match your camera setting</span>
<span class="auto-style1">host        = "192.168.0.10"        # Change to match your camera setting</span>
<span class="auto-style1">resolution  = "1920x1080"           # Resolution</span>
<span class="auto-style1">framerate   =  5                    # Framerate</span>
winname     = "Annotated"           # Window title.

# Step 1: Initialize model with the best available weights
weights = SSDLite320_MobileNet_V3_Large_Weights.DEFAULT
model = ssdlite320_mobilenet_v3_large(weights=weights)
model = model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Capture from camera.
<span class="auto-style1">url = f"rtsp://{user_id}:{user_pw}@{host}/MediaInput/stream_1"  # H.264/H.265</span>
<span class="auto-style1">#url = f"http://{user_id}:{user_pw}@{host}/cgi-bin/nphMotionJpeg?Resolution={resolution}&amp;Quality=Standard&amp;Framerate={framerate}"    # MJPEG</span>
<span class="auto-style1">cap = cv2.VideoCapture(url)</span>

# Exception definition.
BackendError = type('BackendError', (Exception,), {})


def IsWindowVisible(winname):
    '''
    [Abstract]
        Check if the target window exists.
        対象ウィンドウが存在するかを確認する。
    [Param]
        winname :       Window title
    [Return]
        True :          exist
                        存在する
        False :         not exist
                        存在しない
    [Exception]
        BackendError :
    '''
    try:
        ret = cv2.getWindowProperty(winname, cv2.WND_PROP_VISIBLE)
        if ret == -1:
            raise BackendError('Use Qt as backend to check whether window is visible or not.')

        return bool(ret)

    except cv2.error:
        return False


while True:
    # Capture image by opencv.
    ret, orig_image = cap.read()
    if orig_image is None:
        continue

    # Convert image from BGR to RGB.
    rgb_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)

    # Convert image from numpy.ndarray to torchvision image format.
    rgb_image = rgb_image.transpose((2, 0, 1))
    rgb_image = rgb_image / 255.0
    rgb_image = torch.FloatTensor(rgb_image)

    # Step 3: Apply inference preprocessing transforms
    batch = [preprocess(rgb_image)]

    # Step 4: Use the model and visualize the prediction
    with torch.no_grad():
        prediction = model(batch)[0]

    score_threshold = 0.5
    labels = [weights.meta["categories"][class_index] + f": {score_int:.2f}" for class_index, score_int in zip(prediction["labels"], prediction["scores"]) if score_int &gt; score_threshold]
    boxes = prediction["boxes"][prediction["scores"] &gt; score_threshold]

    # Draw result.
    for box, label in zip(boxes, labels):

        cv2.rectangle(
            orig_image,                         # Image.
            (int(box[0]), int(box[1])),         # Vertex of the rectangle.
            (int(box[2]), int(box[3])),         # Vertex of the rectangle opposite to pt1.
            (255, 255, 0),                      # Color.
            4 )                                 # Line type.

        cv2.putText(
            orig_image,                         # Image.
            label,                              # Text string to drawn.
            (int(box[0])+20, int(box[1])+40),   # Bottom-left corner of the text string in the image.
            cv2.FONT_HERSHEY_SIMPLEX,           # Font face. - フォント種別
            0.8,                                # Font scale.
            (255, 0, 255),                      # Color.
            2)                                  # Line type.

    # Display video.
    cv2.imshow(winname, orig_image)

    # Press the "q" key to finish.
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break

    # Exit the program if there is no specified window.
    if not IsWindowVisible(winname):
        break

cap.release()
cv2.destroyAllWindows()</pre>
	<p>&nbsp;</p>
    
  	<p>[動画] プログラムを動作させた様子（i-PRO カメラ、5fps）</p>
    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv3-ssd-ipromini_1.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video> <br>
    
	<p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>軽いとはいえ MobileNetV3 
		もそれなりにCPUを使用します。あなたの環境に合わせてカメラ映像のフレームレートを調節してください。多くの場合で 10fps または 5fps 
		ぐらいに設定する必要があると思います。</p>
        <p>物体検知を別プロセスで処理する（<a href="http://hidetoshi.la.coocan.jp/Programing%20Items/Python/connect_surveillance_camera/connect_with_rtsp.html#4-2._%E9%A1%94%E6%A4%9C%E7%9F%A5%E9%83%A8%E5%88%86%E3%82%92%E5%88%A5%E3%83%97%E3%83%AD%E3%82%BB%E3%82%B9%E3%81%AE%E5%87%A6%E7%90%86%E3%81%AB%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8B">参考記事</a>）ことで映像をフルレート表示するという方法もあります。よろしければ試してみてください。</p>
      </div>
    </div>

    <p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>私の環境では V3 より V1、V2 の方が動作が軽い印象です。V1、V2 
		サンプルは10fpsで動作しましたが、V3 は5fpsまでカメラ設定を下げる必要がありました。</p>
        <p>
		V1、V2サンプルはVOCデータセットでクラス数が20です。一方、V3サンプルはCOCOデータセットでクラス数が91という違いがあります。<br>
		加えて、V1、V2サンプルの入力画像サイズは 224x224 ですが、V3サンプルは 320x320 という違いもあります。そもそもベースとなっているプログラムが異なります。</p>
		<p>以上の理由から、V1、V2、V3 を適正に比較評価できているわけでないこと、ご承知ください。ちゃんと同じ条件で比較したら V3 
		がやはりもっとも高性能という結果になるのではと期待しています。（クラス数の比が演算数にそのまま影響あるとすると概ね妥当な結果かな、と思っています。）</p>
      </div>
    </div>

    <p>&nbsp;</p>
    
</section>
	
<p>&nbsp;</p>
	
<section>
	<h3><a name="参考：_GPU動作させる場合のソースコード修正について0">参考： GPU動作について</a></h3>
	<p>本ページの紹介は Pytorch 動作を "CPU" 
	として説明していますが、私の環境で GPU（CUDA）で動作させた場合の概況について参考記載します。</p>
	<p>上記ソースコードから device を "cuda:0" へ変更するためのソースコード修正を行うことで、私の環境では特にエラーなど発生することなく 
	CUDA 環境で動作させることができました。しかし残念ながら CPU 
	動作からパフォーマンス向上を確認することができませんでした。私の実現方法に課題があるのか、または PyTorch 
	側に問題があるのか、問題の切り分けをまだできておりません。</p>
	<p>進展あればこちらへ追記していきたいと思います。</p>
  
</section>
	
<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<section>
	<h2> <a name="5._学習">5. 学習</a></h2>
	<h4>[概要]</h4>
	<p> あなたが用意した画像データを使って独自の MobileNet-SSD 学習データを作成できるようになることを試みます。</p>
	<p> &nbsp;</p>
	
	<h3><a name="5-1._まずはやってみる_(open_images)">5-1. まずはやってみる (open_images)</a></h3>
	<h4>[概要]</h4>
	<p>今回も <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
	pytorch-ssd</a> で紹介されているプログラムと内容を使って進めます。</p>
	<p>ここでは "<a href="https://github.com/qfgaohao/pytorch-ssd#retrain-on-open-images-dataset" target="_blank">Retrain 
	on Open Images Dataset</a>" で説明されている記述に沿って実際にやってみます。学習済みデータをもとに 銃 
	を認識するようにファインチューニングする、というサンプルのようです。Python 
	プログラムはもちろん、画像データおよびアノテーションデータなど必要なすべての情報を提供いただいているので、最初に取り組みするには最適な内容と思われます。</p>
	<p>&nbsp;</p>
	  <p>使用する学習データ数は以下の通りです。</p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table.] 使用した学習データ数（重複画像を削除後の数です）</caption>
      <thead class="standard_table">
        <tr>
          <th>Class name</th>
          <th width="25%">Train</th>
          <th width="25%">Test</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Hundgun</strong></td>
          <td>545</td>
          <td>68</td>
        </tr>
        <tr>
          <td><strong>Shotgun</strong></td>
          <td>416</td>
          <td>55</td>
        </tr>
        <tr>
          <td><strong>合計</strong></td>
          <td>961</td>
          <td>123</td>
        </tr>
      </tbody>
    </table>
    
      <p>&nbsp;</p>
	<p>&nbsp;</p>
	
	<section>
	<h4>5-1-1. Windows の場合</h4>
	<p>Windows 環境でどこまでできるか不安ですが、作業記録を残しつつ、できるところまで実際にやっていきたいと思います。</p>
	<p>&nbsp;</p>
	<h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.7</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.12.1+cpu</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>22H2</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
    
	<p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
	pytorch-ssd</a> をクローンします。（ここまで読まれた方は恐らく完了しているでしょう）</p>
	<p><a href="#2-1._MobileNetV1-SSD_を動かす（PC 内蔵カメラ）">こちら</a> の記載を参考に実施します。</p>
	<p>&nbsp;</p>
	<p>2. 以下のコマンドで必要なライブラリーをインストールします。これらは後述の "open_images_downloader.py" 
	で使用しているライブラリです。</p>
	<p><span class="cpp-source">pip install <strong>boto3 pandas</strong></span></p>
	<p>&nbsp;</p>
	<p>3. ディレクトリ移動</p>
	<p>ターミナルソフトを起動後、 pytorch-ssd をクローンしたフォルダへ移動（"cd pytorch-ssd" など）します。</p>
	<p>または Explorer で目的フォルダを開いた後、Explorer のアドレスエリアで "cmd" + [Enter] します。</p>
	<p>&nbsp;</p>
	<p>4. まずは再学習済みデータを使ってデモ動作してみます。</p>
	<p><a href="#2-1._MobileNetV1-SSD_を動かす（PC 内蔵カメラ）">こちら</a> 
	  では最初に以下の通り記載されています。</p>
	<pre>wget -P models https://storage.googleapis.com/models-hao/gun_model_2.21.pth
wget -P models https://storage.googleapis.com/models-hao/open-images-model-labels.txt
python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG</pre>
	<p>&nbsp;</p>
	<p>Windows環境では wget を標準で使用できませんので、bitsadmin.exe へ置き換えると最初の２行を以下のような感じで。<br>
	<span class="auto-style3">c:\{作業フォルダ}</span> の部分をご自身の環境に合わせて修正して実行してください。</p>
  <pre>bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/gun_model_2.21.pth <span class="auto-style3">c:\{作業フォルダ}</span>\models\gun_model_2.21.pth
bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/open-images-model-labels.txt <span class="auto-style3">c:\{作業フォルダ}</span>\models\open-images-model-labels.txt
python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt <span class="auto-style3">{テストするJPEGファイル}</span></pre>
	<p>※ 記載の手順で進めても "big.JPG" ファイルはありませんでした。GUN のテストデータを必要としますので、後述の「5. 
	  Download data」を実施後にこの画像ファイルを使って実験した方が良いと思われます。</p>
	<p>&nbsp;</p>
	<p>5. Download data</p>
	<p>インターネットから学習データ一式を取得します。</p>
	<p>
	<a href="https://github.com/qfgaohao/pytorch-ssd#download-data" target="_blank">資料</a>では下記のように書いていますが、Windowsではうまくいかず、エラーになりました。</p>
	<pre>python open_images_downloader.py --root ~/data/open_images --class_names "Handgun,Shotgun" --num_workers 20</pre>
	<p>"--root" 
	の指定フォルダを下記のようにすることで無事ダウンロードすることができました。JPEG画像がそれなりの枚数あるので、私の環境で全データのダウンロードに５分ぐらいかかりました。</p>
	<pre>python open_images_downloader.py --root <strong>./data/open_images</strong> --class_names "Handgun,Shotgun" --num_workers 20</pre>
	<p>&nbsp;</p>
	<p>6. 確認</p>
	<p>ダウンロード完了後の様子を下図に示します。</p>
	<p>指定したフォルダ "./data/open_images" の中に７つの csv ファイルと、３つのフォルダ（test, train, 
	validation）に多くのJPEG画像ファイルを保存していることがわかります。</p>
	<p>
	<img alt="ダウンロード後のフォルダの様子" src="mobilenet-ssd/imgA.jpg" width="800"></p>
	<p>&nbsp;</p>
	<p>"test" フォルダ内の様子です。</p>
	<p><img alt="test フォルダ内の画像(例)" src="mobilenet-ssd/imgF.jpg" width="800"></p>
	<p>&nbsp;</p>
	<p>"class-descriptions-boxable.csv" の様子です。分類するクラス一覧を記述しているようです。</p>
	<p><a href="mobilenet-ssd/img11.jpg" target="_blank">
	<img alt="" class="auto-style6" src="mobilenet-ssd/img11.jpg" width="800"></a></p>
	<p>&nbsp;</p>
	<p>"sub-test-annotations-bbox.csv" の様子です。画像データおよびアノテーションデータ一覧のようです。</p>
	<p>XMin,XMax,YMin,YMax の４つは、画像データ上の物体の位置を示しています。それぞれ 0.0～1.0 
	の範囲で表記するルールとなっているため画像の解像度に影響されません。</p>
	<p>LabelName, id, に記載の情報は、ClassName 
	と紐づけられている情報のようです。"class-descriptions-boxable.csv" で例えば "/m/0gxl3" を検索すると 
	"Handgun" となっています。</p>
	<p><a href="mobilenet-ssd/img12.jpg" target="_blank">
	<img alt="" class="auto-style6" src="mobilenet-ssd/img12.jpg" width="800"></a></p>
	<p>&nbsp;</p>
	<p>以上のような構成でデータを準備することで、あたなや私が独自に学習したい物体と画像についても同様に AI 学習データを作成できることがわかりました。</p>
	<p>&nbsp;</p>
	<p>7. 学習済みモデルを保存</p>
	<p>本ページを上から順に進めてきた人たちは既に models フォルダに学習済みモデルデータを保存済みと思います。まだの方は、<a href="#2-1._MobileNetV1-SSD_を動かす（PC 内蔵カメラ）">こちら</a>に記載の内容に従って 
	"mobilenet-v1-ssd-mp-0_675.pth" をダウンロードしておきます。</p>
	<p>&nbsp;</p>
	<p>8. Retrain (再学習)を実行します</p>
	<p>
	<a href="https://github.com/qfgaohao/pytorch-ssd#retrain" target="_blank">資料</a>では下記のように書いています。</p>
	<pre>python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001  --batch_size 5</pre>
	<p>Windows 環境で動作する場合は少なくとも <span class="cpp-source">--datasets 
	~/data/open_images</span> 
	の部分を実際の環境に合わせて修正したほうがよさそうです。ここでは下記コマンドへ修正して実行してみます。</p>
	<pre>python train_ssd.py --dataset_type open_images --datasets <strong>./data/open_images</strong> --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001  --batch_size 5</pre>
	<p>&nbsp;</p>
	
	<div style="border-radius: 5px; padding: 10px; border: thin solid #C0C0C0; width: 1000px; background-color: #F0F0F0;">
	<p><strong>補足</strong>： </p>
	  <p>上記コマンドに含んでいませんが資料では下記のような説明も記載されていました。</p>
	<p>
	<img alt="" class="border_with_drop-shadow" src="mobilenet-ssd/img1B.jpg" width="800"></p>
	<p>&nbsp;</p>
	<p><span class="cpp-source">--freeze_net</span> を指定することで "prediction head" 
	を除いて全レイヤーをフリーズと記載されています。俗にいう "<strong>転移学習</strong>" 
	を行えそうです。これを指定することで演算が軽くなることを期待できるので、例えばCPU動作でも動作可能なレベルになるか、などを後々評価してみたいところです。</p>
	<p><span class="cpp-source">--freeze_base_net</span> を指定することで "base net 
	layers" をフリーズするようです。こちらも "<strong>転移学習</strong>" のバリエーションの一つですね。本ページの例では、MobileNet 部分をフリーズして SSD or SSD-Lite 
	部分をフリーズしない、と読めば良いのかな、と想像しています。</p>
	  <p>各種ちゃんと動作するようになったらこちらも比較評価してみたいと思います。</p>
	</div>
	
	<p>&nbsp;</p>
	<p>以下、上記コマンド実行後のコンソール出力内容です。</p>
	<p>データ読み込みまでは問題なくできていそうですが、"Start training from epoch 0." の後でエラーとなりました。</p>
	<pre>2022-11-13 15:04:07,989 - root - INFO - Namespace(dataset_type='open_images', datasets=['./data/open_images'], validation_dataset=None, balance_data=False, net='mb1-ssd', freeze_base_net=False, freeze_net=False, mb2_width_mult=1.0, lr=0.01, momentum=0.9, weight_decay=0.0005, gamma=0.1, base_net_lr=0.001, extra_layers_lr=None, base_net=None, pretrained_ssd='models/mobilenet-v1-ssd-mp-0_675.pth', resume=None, scheduler='cosine', milestones='80,100', t_max=100.0, batch_size=5, num_epochs=100, num_workers=4, validation_epochs=5, debug_steps=100, use_cuda=True, checkpoint_folder='models/')
2022-11-13 15:04:07,989 - root - INFO - Prepare training datasets.
2022-11-13 15:04:08,277 - root - INFO - Dataset Summary:Number of Images: 961
Minimum Number of Images for a Class: -1
Label Distribution:
	Handgun: 727
	Shotgun: 580
2022-11-13 15:04:08,282 - root - INFO - Stored labels into file models/open-images-model-labels.txt.
2022-11-13 15:04:08,282 - root - INFO - Train dataset size: 961
2022-11-13 15:04:08,282 - root - INFO - Prepare Validation datasets.
2022-11-13 15:04:08,302 - root - INFO - Dataset Summary:Number of Images: 123
Minimum Number of Images for a Class: -1
Label Distribution:
	Handgun: 81
	Shotgun: 66
2022-11-13 15:04:08,302 - root - INFO - validation dataset size: 123
2022-11-13 15:04:08,302 - root - INFO - Build network.
2022-11-13 15:04:08,349 - root - INFO - Init from pretrained ssd models/mobilenet-v1-ssd-mp-0_675.pth
2022-11-13 15:04:08,391 - root - INFO - Took 0.04 seconds to load the model.
2022-11-13 15:04:08,396 - root - INFO - Learning rate: 0.01, Base net learning rate: 0.001, Extra Layers learning rate: 0.01.
2022-11-13 15:04:08,396 - root - INFO - Uses CosineAnnealingLR scheduler.
2022-11-13 15:04:08,396 - root - INFO - Start training from epoch 0.

C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\optim\lr_scheduler.py:131: <span class="auto-style3"><strong>UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "</strong></span>
Traceback (most recent call last):
  File "C:\Users\kinos\Documents\Github\pytorch-ssd\train_ssd.py", line 325, in &lt;module&gt;
    train(train_loader, net, criterion, optimizer,
  File "C:\Users\kinos\Documents\Github\pytorch-ssd\train_ssd.py", line 116, in train
    for i, data in enumerate(loader):
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 444, in __iter__
    return self._get_iterator()
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 390, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 1077, in __init__
    w.start()
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\context.py", line 336, in _Popen
    return Popen(process_obj)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\popen_spawn_win32.py", line 93, in __init__
    reduction.dump(process_obj, to_child)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
<span class="auto-style3"><strong>AttributeError: Can't pickle local object 'TrainAugmentation.__init__.&lt;locals&gt;.&lt;lambda&gt;'</strong></span>

C:\Users\kinos\Documents\Github\pytorch-ssd&gt;Traceback (most recent call last):
  File "&lt;string&gt;", line 1, in &lt;module&gt;
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\spawn.py", line 126, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input</pre>
	<p>&nbsp;</p>
	<p>"UserWarning: Detected call of `lr_scheduler.step()` before 
	`optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the 
	opposite order: `optimizer.step()` before `lr_scheduler.step()`.&nbsp; 
	Failure to do this will result in PyTorch skipping the first value of the 
	learning rate schedule. See more details at 
	https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate<br>&nbsp; 
	warnings.warn("Detected call of `lr_scheduler.step()` before 
	`optimizer.step()`." について</p>
	<p>こちらは、上記文章中に記載の通り "PyTorch 1.1.0" 以降での仕様変更に伴う警告のようです。</p>
	<p>ソースファイル "train_ssd.py" 中を 323行目 周辺について、下記のとおり変更することで警告されなくなります。</p>
	<p>（修正後）</p>
	<pre class="prettyprint linenums:323 lang-py">
    for epoch in range(last_epoch + 1, args.num_epochs):
        train(train_loader, net, criterion, optimizer,
              device=DEVICE, debug_steps=args.debug_steps, epoch=epoch)
<span class="auto-style1">        scheduler.step()</span>
        
        if epoch % args.validation_epochs == 0 or epoch == args.num_epochs - 1:
            val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)
            logging.info(
                f"Epoch: {epoch}, " +
                f"Validation Loss: {val_loss:.4f}, " +
                f"Validation Regression Loss {val_regression_loss:.4f}, " +
                f"Validation Classification Loss: {val_classification_loss:.4f}"
            )
            model_path = os.path.join(args.checkpoint_folder, f"{args.net}-Epoch-{epoch}-Loss-{val_loss}.pth")
            net.save(model_path)
            logging.info(f"Saved model {model_path}")
  </pre>
	<p>&nbsp;</p>
	<p>（修正前）</p>
	<pre class="prettyprint linenums:323 lang-py">
    for epoch in range(last_epoch + 1, args.num_epochs):
<span class="auto-style1">        scheduler.step()</span>
        train(train_loader, net, criterion, optimizer,
              device=DEVICE, debug_steps=args.debug_steps, epoch=epoch)
        
        if epoch % args.validation_epochs == 0 or epoch == args.num_epochs - 1:
            val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)
            logging.info(
                f"Epoch: {epoch}, " +
                f"Validation Loss: {val_loss:.4f}, " +
                f"Validation Regression Loss {val_regression_loss:.4f}, " +
                f"Validation Classification Loss: {val_classification_loss:.4f}"
            )
            model_path = os.path.join(args.checkpoint_folder, f"{args.net}-Epoch-{epoch}-Loss-{val_loss}.pth")
            net.save(model_path)
            logging.info(f"Saved model {model_path}")
  </pre>
	<p>&nbsp;</p>
	<p>（修正後）のプログラムを再度実行すると、とりあえず１つ目の警告（UserWarning）は消えました。</p>
	<pre>C:\Users\kinos\Documents\Github\pytorch-ssd&gt;python train_ssd.py --dataset_type open_images --datasets ./data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001  --batch_size 5
2022-11-13 16:35:24,380 - root - INFO - Namespace(dataset_type='open_images', datasets=['./data/open_images'], validation_dataset=None, balance_data=False, net='mb1-ssd', freeze_base_net=False, freeze_net=False, mb2_width_mult=1.0, lr=0.01, momentum=0.9, weight_decay=0.0005, gamma=0.1, base_net_lr=0.001, extra_layers_lr=None, base_net=None, pretrained_ssd='models/mobilenet-v1-ssd-mp-0_675.pth', resume=None, scheduler='cosine', milestones='80,100', t_max=100.0, batch_size=5, num_epochs=100, num_workers=4, validation_epochs=5, debug_steps=100, use_cuda=True, checkpoint_folder='models/')
2022-11-13 16:35:24,380 - root - INFO - Prepare training datasets.
2022-11-13 16:35:24,662 - root - INFO - Dataset Summary:Number of Images: 961
Minimum Number of Images for a Class: -1
Label Distribution:
        Handgun: 727
        Shotgun: 580
2022-11-13 16:35:24,662 - root - INFO - Stored labels into file models/open-images-model-labels.txt.
2022-11-13 16:35:24,662 - root - INFO - Train dataset size: 961
2022-11-13 16:35:24,662 - root - INFO - Prepare Validation datasets.
2022-11-13 16:35:24,693 - root - INFO - Dataset Summary:Number of Images: 123
Minimum Number of Images for a Class: -1
Label Distribution:
        Handgun: 81
        Shotgun: 66
2022-11-13 16:35:24,693 - root - INFO - validation dataset size: 123
2022-11-13 16:35:24,693 - root - INFO - Build network.
2022-11-13 16:35:24,743 - root - INFO - Init from pretrained ssd models/mobilenet-v1-ssd-mp-0_675.pth
2022-11-13 16:35:24,774 - root - INFO - Took 0.03 seconds to load the model.
2022-11-13 16:35:24,789 - root - INFO - Learning rate: 0.01, Base net learning rate: 0.001, Extra Layers learning rate: 0.01.
2022-11-13 16:35:24,789 - root - INFO - Uses CosineAnnealingLR scheduler.
2022-11-13 16:35:24,789 - root - INFO - Start training from epoch 0.
Traceback (most recent call last):
  File "C:\Users\kinos\Documents\Github\pytorch-ssd\train_ssd.py", line 324, in &lt;module&gt;
    train(train_loader, net, criterion, optimizer,
  File "C:\Users\kinos\Documents\Github\pytorch-ssd\train_ssd.py", line 116, in train
    for i, data in enumerate(loader):
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 444, in __iter__
    return self._get_iterator()
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 390, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 1077, in __init__
    w.start()
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\context.py", line 336, in _Popen
    return Popen(process_obj)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\popen_spawn_win32.py", line 93, in __init__
    reduction.dump(process_obj, to_child)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
<span class="auto-style3"><strong>AttributeError: Can't pickle local object 'TrainAugmentation.__init__.&lt;locals&gt;.&lt;lambda&gt;'</strong></span>

C:\Users\kinos\Documents\Github\pytorch-ssd&gt;Traceback (most recent call last):
  File "&lt;string&gt;", line 1, in &lt;module&gt;
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\spawn.py", line 126, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input</pre>
	<p>&nbsp;</p>
	<p>次に残るエラーを解決します。</p>
	<p>この問題を解決している記事がありました。</p>
	<p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>上記問題のうち 「AttributeError: Can't pickle local object 
		'TrainAugmentation.__init__.&lt;locals&gt;.&lt;lambda&gt;'」 については下記ページで議論されてそうです。</p>
		<p>
		<a href="https://github.com/qfgaohao/pytorch-ssd/issues/71" target="_blank">
		AttributeError: Can't pickle local object 
		'TrainAugmentation.__init__.&lt;locals&gt;.&lt;lambda&gt;' · Issue #71 · 
		qfgaohao/pytorch-ssd (github.com)</a></p>
      </div>
    </div>

    <p>&nbsp;</p>
	<p>上記URLに記載の内容に従って修正した後の「vision/ssd/data_preprocessing.py」を以下に記載します。<br>
	修正した場所を色付けしています。</p>
	<p>&nbsp;</p>
	<p>"vision/ssd/data_preprocessing.py"</p>
	<pre class="prettyprint linenums lang-py">from ..transforms.transforms import *


<span class="auto-style1">class ScaleByStd:</span>
<span class="auto-style1">    def __init__(self, std: float):</span>
<span class="auto-style1">        self.std = std</span>

<span class="auto-style1">    def __call__(self, img, boxes=None, labels=None):</span>
<span class="auto-style1">        return (img / self.std, boxes, labels)</span>


class TrainAugmentation:
    def __init__(self, size, mean=0, std=1.0):
        """
        Args:
            size: the size the of final image.
            mean: mean pixel value per channel.
        """
        self.mean = mean
        self.size = size
        self.augment = Compose([
            ConvertFromInts(),
            PhotometricDistort(),
            Expand(self.mean),
            RandomSampleCrop(),
            RandomMirror(),
            ToPercentCoords(),
            Resize(self.size),
            SubtractMeans(self.mean),
<span class="auto-style1">            #lambda img, boxes=None, labels=None: (img / std, boxes, labels),</span>
<span class="auto-style1">            ScaleByStd(std),</span>
            ToTensor(),
        ])

    def __call__(self, img, boxes, labels):
        """

        Args:
            img: the output of cv.imread in RGB layout.
            boxes: boundding boxes in the form of (x1, y1, x2, y2).
            labels: labels of boxes.
        """
        return self.augment(img, boxes, labels)


class TestTransform:
    def __init__(self, size, mean=0.0, std=1.0):
        self.transform = Compose([
            ToPercentCoords(),
            Resize(size),
            SubtractMeans(mean),
<span class="auto-style1">            #lambda img, boxes=None, labels=None: (img / std, boxes, labels),</span>
<span class="auto-style1">            ScaleByStd(std),</span>
            ToTensor(),
        ])

    def __call__(self, image, boxes, labels):
        return self.transform(image, boxes, labels)


class PredictionTransform:
    def __init__(self, size, mean=0.0, std=1.0):
        self.transform = Compose([
            Resize(size),
            SubtractMeans(mean),
<span class="auto-style1">            #lambda img, boxes=None, labels=None: (img / std, boxes, labels),</span>
<span class="auto-style1">            ScaleByStd(std),</span>
            ToTensor()
        ])

    def __call__(self, image):
        image, _, _ = self.transform(image)
        return image</pre>
	<p>&nbsp;</p>
	<p>上記修正を行うことで、下記コマンドを正常に実行できるようになりました。</p>
	<p>&nbsp;</p>
	<pre>python train_ssd.py --dataset_type open_images --datasets <strong>./data/open_images</strong> --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001  --batch_size 5</pre>
	<p>&nbsp;</p>
	<p>100 Epoch を約５時間半で学習できました。（CPU動作の場合です。GPU動作の場合はきっともっと高速に動作するでしょう。）</p>
	<p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
		    <p>train_ssd.py を正常に実行できるようにはなりましたが、私の環境では下記警告？が頻繁に出力されます。</p>
        <p><span class="cpp-source">pytorch-ssd\vision\transforms\transforms.py:247: 
		VisibleDeprecationWarning: Creating an ndarray from ragged nested 
		sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with 
		different lengths or shapes) is deprecated. If you meant to do this, you 
		must specify 'dtype=object' when creating the ndarray.<br>&nbsp; mode = 
		random.choice(self.sample_options)</span></p>
		<p>このままでも正常に動作するようですが、修正したい場合は下記URLの記事を参照。</p>
		<p>
		<a href="https://github.com/amdegroot/ssd.pytorch/issues/498" target="_blank">VisibleDeprecationWarning of augmentations.py · Issue #498 · 
		amdegroot/ssd.pytorch (github.com)</a></p>
      </div>
    </div>

    <p>&nbsp;</p>
	<p>"transforms.py" の 247行目を以下のように修正すればよいようです。</p>
	<pre class="prettyprint linenums lang-py" style="width: 800px">
# before
mode = random.choice(self.sample_options)

# after
random_idx = random.randint(0, len(self.sample_options) - 1)
mode = self.sample_options[random_idx]</pre>
	<p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>"models\open-images-model-labels.txt" は "train_ssd.py" 
		を実行することで自動的に生成されるようにプログラミングされているようです。</p>
        <p>dataset_type == 'voc' の場合は "voc-model-labels.txt" を自動的に生成するようです。</p>
		<p>２つの生成方法は細かい部分でいろいろと差異がありそうです。詳細については "train_ssd.py" を参照してください。</p>
      </div>
    </div>

    <p>&nbsp;</p>
	  <p>&nbsp;</p>
	<p>9. 学習したデータとサンプル画像を使用して Gun の認識をしてみます。</p>
	<p>私の学習済みデータとサンプル画像を使った場合の例を以下に記載します。あなたが実際に使用するファイル名およびパスへ修正してください。</p>
	
	
	<pre>python run_ssd_example.py mb1-ssd models\mb1-ssd-Epoch-10-Loss-2.9762498426437376.pth models\open-images-model-labels.txt data\open_images\validation\018d4a117c1d7e16.jpg</pre>
	<p>&nbsp;</p>
	<p>ただし run_ssd_example.py は OpenSSL 
	のバージョンアップに伴って一部修正する必要があります。修正後のソースコードを下記に示します。<br>60, 64行目の部分で６か所 int 
	キャストを追加する必要があります。</p>
	<p>&nbsp;</p>
	<p>"run_ssd_example.py"</p>
	<pre class="prettyprint linenums lang-py">from vision.ssd.vgg_ssd import create_vgg_ssd, create_vgg_ssd_predictor
from vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd, create_mobilenetv1_ssd_predictor
from vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite, create_mobilenetv1_ssd_lite_predictor
from vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite, create_squeezenet_ssd_lite_predictor
from vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite, create_mobilenetv2_ssd_lite_predictor
from vision.ssd.mobilenetv3_ssd_lite import create_mobilenetv3_large_ssd_lite, create_mobilenetv3_small_ssd_lite
from vision.utils.misc import Timer
import cv2
import sys


if len(sys.argv) &lt; 5:
    print('Usage: python run_ssd_example.py &lt;net type&gt;  &lt;model path&gt; &lt;label path&gt; &lt;image path&gt;')
    sys.exit(0)
net_type = sys.argv[1]
model_path = sys.argv[2]
label_path = sys.argv[3]
image_path = sys.argv[4]

class_names = [name.strip() for name in open(label_path).readlines()]

if net_type == 'vgg16-ssd':
    net = create_vgg_ssd(len(class_names), is_test=True)
elif net_type == 'mb1-ssd':
    net = create_mobilenetv1_ssd(len(class_names), is_test=True)
elif net_type == 'mb1-ssd-lite':
    net = create_mobilenetv1_ssd_lite(len(class_names), is_test=True)
elif net_type == 'mb2-ssd-lite':
    net = create_mobilenetv2_ssd_lite(len(class_names), is_test=True)
elif net_type == 'mb3-large-ssd-lite':
    net = create_mobilenetv3_large_ssd_lite(len(class_names), is_test=True)
elif net_type == 'mb3-small-ssd-lite':
    net = create_mobilenetv3_small_ssd_lite(len(class_names), is_test=True)
elif net_type == 'sq-ssd-lite':
    net = create_squeezenet_ssd_lite(len(class_names), is_test=True)
else:
    print("The net type is wrong. It should be one of vgg16-ssd, mb1-ssd and mb1-ssd-lite.")
    sys.exit(1)
net.load(model_path)

if net_type == 'vgg16-ssd':
    predictor = create_vgg_ssd_predictor(net, candidate_size=200)
elif net_type == 'mb1-ssd':
    predictor = create_mobilenetv1_ssd_predictor(net, candidate_size=200)
elif net_type == 'mb1-ssd-lite':
    predictor = create_mobilenetv1_ssd_lite_predictor(net, candidate_size=200)
elif net_type == 'mb2-ssd-lite' or net_type == "mb3-large-ssd-lite" or net_type == "mb3-small-ssd-lite":
    predictor = create_mobilenetv2_ssd_lite_predictor(net, candidate_size=200)
elif net_type == 'sq-ssd-lite':
    predictor = create_squeezenet_ssd_lite_predictor(net, candidate_size=200)
else:
    predictor = create_vgg_ssd_predictor(net, candidate_size=200)

orig_image = cv2.imread(image_path)
image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)
boxes, labels, probs = predictor.predict(image, 10, 0.4)

for i in range(boxes.size(0)):
    box = boxes[i, :]
    cv2.rectangle(orig_image, (<span class="auto-style1">int(box[0])</span>, <span class="auto-style1">int(box[1])</span>), (<span class="auto-style1">int(box[2])</span>, <span class="auto-style1">int(box[3])</span>), (255, 255, 0), 4)
    #label = f"""{voc_dataset.class_names[labels[i]]}: {probs[i]:.2f}"""
    label = f"{class_names[labels[i]]}: {probs[i]:.2f}"
    cv2.putText(orig_image, label,
                (<span class="auto-style1">int(box[0])</span> + 20, <span class="auto-style1">int(box[1])</span> + 40),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,  # font scale
                (255, 0, 255),
                2)  # line type
path = "run_ssd_example_output.jpg"
cv2.imwrite(path, orig_image)
print(f"Found {len(probs)} objects. The output image is {path}")</pre>
	<p>&nbsp;</p>
	
	<p>こちら Epoch-10 の学習データによる結果です。評価に使用した画像は上記手順でダウンロードした 
	<strong class="cpp-source">varidation</strong> フォルダ中にある画像（validation\018d4a117c1d7e16.jpg）を使用しました。</p>
	  <p>まだ認識精度は低そうです。</p>
	<p>	<img alt="再学習データを使って Gun を認識" src="mobilenet-ssd/run_ssd_example_output_Epoch-10.jpg" width="600"></p>
	<p>&nbsp;</p>
	<p>こちら Epoch-30 の学習データによる結果です。大分良い感じになってきました。</p>
	<p>	<img alt="再学習データを使って Gun を認識" src="mobilenet-ssd/run_ssd_example_output_Epoch-30.jpg" width="600"></p>
	<p>&nbsp;</p>
	<p>こちら Epoch-99 の学習データによる結果です。これが最終データです。スコアも 0.92 まで伸びました。良い感じです。</p>
	<p>	<img alt="再学習データを使って Gun を認識" src="mobilenet-ssd/run_ssd_example_output_Epoch-99.jpg" width="600"></p>
	
	<p>&nbsp;</p>
	  <p>&nbsp;</p>
	  <p>以下、学習時の引数を変更したときの学習時間、代表的な Epoch における "Validation Loss" の表です。<br>
	  AI学習は乱数を使っている部分もあるので、私と同じ手順を行っても同じ結果になりません。私自身が同じことを複数回行っても異なる結果になったりします。あくまで参考値ということで。</p>
	  <p>&nbsp;</p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table.] 引数毎の 学習時間、Validation Loss 比較</caption>
      <thead class="standard_table">
        <tr>
          <th style="white-space: nowrap">Epoch num.</th>
          <th width="25%" style="white-space: nowrap">Normal<br>(追加引数無し)</th>
          <th width="25%" style="white-space: nowrap">--freeze_net</th>
          <th width="25%" style="white-space: nowrap">--freeze_base_net</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>0</strong></td>
          <td>3.77</td>
          <td>8.19</td>
          <td>4.63</td>
        </tr>
        
        <tr>
          <td><strong>10</strong></td>
          <td>2.98</td>
          <td>7.98</td>
          <td>4.10</td>
        </tr>
        
        <tr>
          <td><strong>20</strong></td>
          <td>2.98</td>
          <td>8.02</td>
          <td>3.95</td>
        </tr>
        
        <tr>
          <td><strong>30</strong></td>
          <td>2.82</td>
          <td>6.27</td>
          <td>3.76</td>
        </tr>
        
        <tr>
          <td><strong>40</strong></td>
          <td>2.75</td>
          <td>5.91</td>
          <td>3.59</td>
        </tr>
        
        <tr>
          <td><strong>50</strong></td>
          <td>3.00</td>
          <td>4.92</td>
          <td>3.49</td>
        </tr>
        
        <tr>
          <td><strong>60</strong></td>
          <td>2.83</td>
          <td>4.45</td>
          <td>3.25</td>
        </tr>
        
        <tr>
          <td><strong>70</strong></td>
          <td>2.89</td>
          <td>3.83</td>
          <td>3.17</td>
        </tr>
        
        <tr>
          <td><strong>80</strong></td>
          <td>2.80</td>
          <td>3.58</td>
          <td>3.07</td>
        </tr>
        
        <tr>
          <td><strong>90</strong></td>
          <td>2.86</td>
          <td>3.42</td>
          <td>3.01</td>
        </tr>
        
        <tr>
          <td><strong>99</strong></td>
          <td>2.84</td>
          <td>3.40</td>
          <td>3.01</td>
        </tr>
        
        <tr>
          <td><strong>学習時間</strong></td>
          <td style="white-space: nowrap">5時間30分</td>
          <td style="white-space: nowrap">2時間20分</td>
          <td style="white-space: nowrap">2時間40分</td>
        </tr>
        
      </tbody>
    </table>
    
      <p>&nbsp;</p>
	  <p>以下、各引数における Epoch 99 の学習データで認識させてみた結果比較です。</p>
	  <p>
	  <img alt="normal" src="mobilenet-ssd/run_ssd_example_output_Epoch-99.jpg" width="600"></p>
	  <p>[Fig. normal, Eopch 99]</p>
	  <p>&nbsp;</p>
	  <p>
	  <img alt="freeze_net" src="mobilenet-ssd/gun_freeze_net_epoch-99.jpg" width="600"></p>
	  <p>[Fig. --freeze_net, Epoch 99]</p>
	  <p>&nbsp;</p>
	  <p>
	  <img alt="freeze_base_net" src="mobilenet-ssd/gun_freeze_base_net_epoch-99.jpg" width="600"></p>
	  <p>[Fig. --freeze_base_net, Epoch 99]</p>
	  <p>&nbsp;</p>
	  <p>Normal が最も良い感じの認識結果になりました。一方、"--freeze_base_net" 
	  は学習時間が約半分に短縮できたうえで認識結果もそこそこ良い感じでした。<br>
	  この２つの方式を使い分けてみても良いかもしれません、という感じの結果になりました。</p>
	  <p>&nbsp;</p>
	</section>
	
	<p>&nbsp;</p>
	
	<section>
	<h4>5-1-2. Linux の場合</h4>
	<p>&nbsp;</p>
	<p>To Be Edited. </p>
	<p>&nbsp;</p>
	</section>
	
</section>
	
<p>&nbsp;</p>

<section>
<h3><a name="5-2._独自の画像を学習してみる_(VOC)">5-2. 独自の画像を学習してみる (VOC)</a></h3>
	<h4>[概要]</h4>
  <p>今度は 自身で集めた画像を使ってオリジナルの学習データによる学習を試みます。</p>
  <p>前節で使用した train_ssd.py は入力データ型として "open_images" と "voc" の２種類に対応しているようです。"open_images" に対応したアノテーションツールで良さそうなものを見つけられなかったので、"voc" 
  型の出力に対応していて有名なオープンソースツールである 
  <a href="https://github.com/heartexlabs/labelImg" target="_blank">LabelImg</a> というツールを使用してアノテーションデータを作成してみます。</p>
  <p>少なめのデータ＆認識精度度外視で、まずは技術的に実現できることまでを目標に進めてみます。</p>
  <p>&nbsp;</p>
	  <p>使用する学習データ数は以下の通りです。</p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table.] 使用した学習データ数（重複画像を削除後の数です）</caption>
      <thead class="standard_table">
        <tr>
          <th>Class name</th>
          <th width="25%">Train</th>
          <th width="25%">Test</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>&nbsp;</td>
          <td>&nbsp;</td>
          <td>&nbsp;</td>
        </tr>
        <tr>
          <td>&nbsp;</td>
          <td>&nbsp;</td>
          <td>&nbsp;</td>
        </tr>
        <tr>
          <td><strong>合計</strong></td>
          <td>&nbsp;</td>
          <td>&nbsp;</td>
        </tr>
      </tbody>
    </table>
    
      <p>&nbsp;</p>
  <p>&nbsp;</p>
  <p>1. "train_ssd.py" が想定する VOC データセットのフォルダ・ファイルを準備</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
  <p>2. 学習用の画像を集めて "JPEGImages" フォルダへ保存</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
  <p>3. LabelImg をインストール（必要に応じて修正）</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
  <p>4. LabelImg を起動、設定</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
  <p>5. LabelImg を使ってアノテーション実施</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
  <p>6. "ImageSets\Main\trainval.txt" を設定</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
  <p>7. "ImageSets\Main\tet.txt" を設定</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
  <p>8. "labels.txt" を設定</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
  <p>9. 学習を実施</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
</section>
<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<section>
  <h2><a name="ライセンス">ライセンス</a></h2>
<p>本ページの情報は、特記無い限り下記 MIT ライセンスで提供されます。</p>
<table class="border-collapse" style="width: 600px; background-color: #F0F0F0; word-break: break-word;">
  <tr>
    <td>
The MIT License (MIT)<br><br>

Copyright © 2022 Kinoshita Hidetoshi<br><br>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:<br><br>

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.<br><br>

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
    </td>
  </tr>
</table>
  <p>&nbsp;</p>
  <p>&nbsp;</p>
</section>

<br>

<section>
	<h2><a name="参考">参考</a></h2>
	<ul>
		<li>[1] PyTorch<br><a href="https://pytorch.org/" target="_blank">
      https://pytorch.org/</a></li>
    <li>[2] qfgaohao/pytorch-ssd: MobileNetV1, MobileNetV2, VGG based 
      SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box 
      support for retraining on Open Images dataset. ONNX and Caffe2 support. 
      Experiment Ideas like CoordConv. (github.com)<br>
      <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
      https://github.com/qfgaohao/pytorch-ssd</a></li>
    <li>[3] PyTorchでMobileNet SSDによるリアルタイム物体検出｜はやぶさの技術ノート (cpp-learning.com)<br>
      <a href="https://cpp-learning.com/pytorch_mobilenet-ssd/" target="_blank">
      https://cpp-learning.com/pytorch_mobilenet-ssd/</a></li>
    <li>[4] MobilenetSSD : 高速に物体検出を行う機械学習モデル. ailia… | by Kazuki Kyakuno | axinc | Medium<br>
      <a href="https://medium.com/axinc/mobilenetssd-高速に物体検出を行う機械学習モデル-be3ca37c411" target="_blank">
      https://medium.com/axinc/mobilenetssd-高速に物体検出を行う機械学習モデル-be3ca37c411</a></li>
		<li>[5] Everything You Need To Know About Torchvision’s SSDlite Implementation | PyTorch<br>
      <a href="https://pytorch.org/blog/torchvision-ssdlite-implementation/" target="_blank">
      https://pytorch.org/blog/torchvision-ssdlite-implementation/</a></li>
		<li>[6] vision/ssdlite.py at b6f733046c9259f354d060cd808241a558d7d596 · pytorch/vision · GitHub<br>
  		<a href="https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L159-L162" target="_blank">
	  	https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L159-L162</a></li>
		<li>[7] Models and pre-trained weights — Torchvision main documentation (pytorch.org)<br>
		  <a href="https://pytorch.org/vision/main/models.html" target="_blank">
		  https://pytorch.org/vision/main/models.html</a></li>
		<li>[8] Visualization utilities — Torchvision 0.11.0 documentation (pytorch.org)<br>
      <a href="https://pytorch.org/vision/0.11/auto_examples/plot_visualization_utils.html" target="_blank">
      https://pytorch.org/vision/0.11/auto_examples/plot_visualization_utils.html</a></li>
		<li>[9] PyTorchでObeject Detection (mashykom.com)<br>
      <a href="https://www.koi.mashykom.com/pytorch2.html" target="_blank">
      https://www.koi.mashykom.com/pytorch2.html</a></li>
		<li>[10] SSDLite MobileNetV3 Backbone Object Detection with PyTorch and 
		Torchvision - DebuggerCafe<br>
		<a href="https://debuggercafe.com/ssdlite-mobilenetv3-backbone-object-detection-with-pytorch-and-torchvision/" target="_blank">
		https://debuggercafe.com/ssdlite-mobilenetv3-backbone-object-detection-with-pytorch-and-torchvision/</a></li>
		<li>[11] MobileNets: Efficient Convolutional Neural Networks for Mobile 
		Vision Applications <br>
		<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank">
		https://arxiv.org/pdf/1704.04861.pdf</a></li>
		<li>[12] MobileNetV2: Inverted Residuals and Linear Bottlenecks <br>
		<a href="https://arxiv.org/pdf/1801.04381v3.pdf" target="_blank">
		https://arxiv.org/pdf/1801.04381v3.pdf</a></li>
		<li>[13] Searching for MobileNetV3 <br>
		<a href="https://arxiv.org/pdf/1905.02244.pdf" target="_blank">
		https://arxiv.org/pdf/1905.02244.pdf</a></li>
		<li>[14] PyTorchによるSSD Mobilenetでの転移学習（Jetson Nano） | そう備忘録 
		(souichi.club)<br>
		<a href="https://www.souichi.club/deep-learning/transfer-learning/" target="_blank">
		https://www.souichi.club/deep-learning/transfer-learning/</a></li>
		<li>[15] PyTorch 新たなクラスの物体検出をSSDでやってみる | cedro-blog (cedro3.com)<br>
		<a href="http://cedro3.com/ai/pytorch-ssd-bccd/" target="_blank">
		http://cedro3.com/ai/pytorch-ssd-bccd/</a></li>
		<li>[16] MobilenetSSD : 高速に物体検出を行う機械学習モデル. ailia… | by Kazuki Kyakuno | 
		axinc | Medium<br>
		<a href="https://medium.com/axinc/mobilenetssd-高速に物体検出を行う機械学習モデル-be3ca37c411" target="_blank">
		https://medium.com/axinc/mobilenetssd-高速に物体検出を行う機械学習モデル-be3ca37c411</a></li>
		<li>[17] 画像を扱う機械学習のためのデータセットまとめ - Qiita<br>
		<a href="https://qiita.com/leetmikeal/items/7c0d23e39bf38ab8be23" target="_blank">
		https://qiita.com/leetmikeal/items/7c0d23e39bf38ab8be23</a></li>
		<li>[18]
		<a href="https://udemy.benesse.co.jp/data-science/deep-learning/transfer-learning.html#:~:text=%E8%BB%A2%E7%A7%BB%E5%AD%A6%E7%BF%92%E3%81%A8%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E9%81%95%E3%81%84%E3%81%AF%EF%BC%9F%20%E8%BB%A2%E7%A7%BB%E5%AD%A6%E7%BF%92%E3%81%A8%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AF%E3%80%81%E3%81%A9%E3%81%A1%E3%82%89%E3%82%82%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%9F%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E6%89%8B%E6%B3%95%E3%81%A7%E3%81%99%E3%80%82%20%E3%82%88%E3%81%8F%E6%B7%B7%E5%90%8C%E3%81%95%E3%82%8C%E3%81%A6%E3%81%97%E3%81%BE%E3%81%84%E3%81%BE%E3%81%99%E3%81%8C%E3%80%81%E3%81%93%E3%81%AE2%E3%81%A4%E3%81%AE%E6%89%8B%E6%B3%95%E3%81%AF%E7%95%B0%E3%81%AA%E3%82%8A%E3%81%BE%E3%81%99%E3%80%82,%E3%81%9D%E3%82%8C%E3%81%9E%E3%82%8C%E3%81%AE%E9%81%95%E3%81%84%E3%82%92%E8%A6%8B%E3%81%A6%E3%81%84%E3%81%8D%E3%81%BE%E3%81%97%E3%82%87%E3%81%86%E3%80%82%20%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AF%E3%80%81%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%B1%A4%E3%81%AE%E9%87%8D%E3%81%BF%E3%82%92%E5%BE%AE%E8%AA%BF%E6%95%B4%E3%81%99%E3%82%8B%E6%89%8B%E6%B3%95%E3%81%A7%E3%81%99%E3%80%82%20%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E9%87%8D%E3%81%BF%E3%82%92%E5%88%9D%E6%9C%9F%E5%80%A4%E3%81%A8%E3%81%97%E3%80%81%E5%86%8D%E5%BA%A6%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AB%E3%82%88%E3%81%A3%E3%81%A6%E5%BE%AE%E8%AA%BF%E6%95%B4%E3%81%97%E3%81%BE%E3%81%99%E3%80%82%20%E8%BB%A2%E7%A7%BB%E5%AD%A6%E7%BF%92%E3%81%AF%E3%80%81%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E9%87%8D%E3%81%BF%E3%81%AF%E5%9B%BA%E5%AE%9A%E3%81%97%E3%80%81%E8%BF%BD%E5%8A%A0%E3%81%97%E3%81%9F%E5%B1%A4%E3%81%AE%E3%81%BF%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%A6%E5%AD%A6%E7%BF%92%E3%81%97%E3%81%BE%E3%81%99%E3%80%82">
		転移学習とは？ディープラーニングで期待の「転移学…｜Udemy メディア (benesse.co.jp)</a></li>
	</ul>
	<p>&nbsp;</p>
	<p>アノテーションツール</p>
	<ul>
		<li>heartexlabs/labelImg<br>
		<a href="https://github.com/heartexlabs/labelImg" target="_blank">
		https://github.com/heartexlabs/labelImg</a></li>
		<li>AIアノテーションツール20選を比較！タグ付け自動化ツールの選び方を紹介 (aismiley.co.jp)<br>
		<a href="https://aismiley.co.jp/ai_news/3-tools-to-perform-overlay-indispensable-for-machine-learning/" target="_blank">
		https://aismiley.co.jp/ai_news/3-tools-to-perform-overlay-indispensable-for-machine-learning/</a><br></li>
	</ul>
</section>

<p>&nbsp;</p>

<hr>

<p>&nbsp;</p>

<section>
	<h2 style="margin-bottom:5px">変更履歴</h2>
	<table>
	  <tr>
	    <td class="td_history_date">2023-01-03</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">「<a href="#5._学習">5. 学習</a>」内の記事を追記。Windowsで学習できるようになりました。</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">2022-11-13</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">「<a href="#5._学習">5. 学習</a>」を追加。ただし記載途中</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">2022-09-19</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">「<a href="#4-2._MobileNetV3-SSD-Lite_を動かす（PC_内蔵カメラ）">4-2. MobileNetV3-SSD-Lite を動かす（PC 内蔵カメラ）</a>」を追加</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">&nbsp;</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">「<a href="#4-3._MobileNetV3-SSD-Lite_を動かす（i-PRO_内蔵カメラ）">4-3. MobileNetV3-SSD-Lite を動かす（i-PRO カメラ）</a>」を追加</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">&nbsp;</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">論文などの技術情報について追記</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">2022-09-16</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">「<a href="#4-1._MobileNetV3-SSD-Lite_を動かす（JPEGファイル）">4-1. MobileNetV3-SSD-Lite を動かす（JPEGファイル）</a>」を追加</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">2022-08-30</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">「<a href="#2-2._MobileNetV1-SSD_を動かす（i-PRO_カメラ）">2-2. MobileNetV1-SSD を動かす（i-PRO カメラ）</a>」を追加</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">&nbsp;</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">「<a href="#3-1._MobileNetV2-SSD-Lite_を動かす（PC_内蔵カメラ）">3-1. MobileNetV2-SSD-Lite を動かす（PC 内蔵カメラ）</a>」を追加</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">&nbsp;</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">「<a href="#3-2._MobileNetV2-SSD-Lite_を動かす（i-PRO_カメラ）">3-2. MobileNetV2-SSD-Lite を動かす（i-PRO カメラ）</a>」を追加</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">2022-08-03</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">"Pull Request #178" がマージされたことを追記</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">2022-06-26</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">新規作成 </td>
	  </tr>
	</table>
</section>

<p>&nbsp;</p>

<section>
<p><a href="../../index.html" target="_parent">Programming Items トップページ</a></p>
<p><a href="../../privacy_policy.html">プライバシーポリシー</a></p>
</section>

<p>&nbsp;</p>

<footer>
	<p><small>Copyright © 2022 Kinoshita Hidetoshi (木下英俊)</small></p>
</footer>

</body>
</html>
