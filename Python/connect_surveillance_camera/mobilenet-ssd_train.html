<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="kinoshita hidetoshi (木下 英俊)">
  <meta name="description" content="木下英俊が自身のためにプログラムメモを残すことを目的に作成したページです。">
  <meta name="keywords" content="">
  <!-- キャッシュ無効化 -->
  <meta http-equiv="Cache-Control" content="no-cache">
	
  <!-- タイトル -->
  <title>物体検知 － MobileNet-SSD (学習編) | Programing Items</title>
	
  <!-- ファビコン -->
  <link rel="shortcut icon" href="../../favicon.ico">
  
  <!-- CSS -->
  <link href="https://unpkg.com/ress/dist/ress.min.css" rel="stylesheet">
	<link rel="stylesheet" href="../../design.css" type="text/css">
  
	<!-- Start for 'google-code-prettify' -->
	<link href="../../prettify/styles/desert.css" rel="stylesheet" type="text/css">
	<script src="../../prettify/prettify.js" type="text/javascript"></script>
	<!-- End for 'google-code-prettify' -->	
	
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-V2DZQK54C2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-V2DZQK54C2');
  </script>
  <!-- Global site tag (gtag.js) - Google Analytics -->

  <style type="text/css">
  .auto-style1 {
    background-color: #505000;
  }
  .auto-style2 {
    text-decoration: underline;
  }
  .auto-style3 {
    color: #FF0000;
  }
  .auto-style6 {
	  border-width: 0px;
  }
  .auto-style7 {
	border-width: 1px;
  }
  </style>
  
</head>

<body onload="prettyPrint();">
	
<h1>物体検知 － MobileNet-SSD (学習編)</h1>

<p> &nbsp;</p>
<div class="mokuji">
  <nav>
    <h2>目次</h2>
	<p>=== 1-4章は別ページ「<a href="mobilenet-ssd.html">物体検知 － MobileNet-SSD (推論編)</a>」で記載の記事です 
	===</p>
    <p>1. 準備</p>
    <p>&nbsp;&nbsp; 1-1. PyTorch をインストールする</p>
    <p>&nbsp;&nbsp; 1-2. 必要なライブラリをインストールする</p>
    <p>2. MobileNetV1-SSD</p>
    <p>&nbsp;&nbsp; 2-1. MobileNetV1-SSD を動かす（PC 内蔵カメラ）</p>
    <p>&nbsp;&nbsp; 2-2. MobileNetV1-SSD を動かす（i-PRO カメラ）</p>
    <p>3. MobileNetV2-SSD-Lite</p>
    <p>&nbsp;&nbsp; 3-1. MobileNetV2-SSD-Lite を動かす（PC 内蔵カメラ）</p>
    <p>&nbsp;&nbsp; 3-2. MobileNetV2-SSD-Lite を動かす（i-PRO カメラ）</p>
    <p>&nbsp;&nbsp; 参考： GPU動作させる場合のソースコード修正について</p>
    <p>4. MobileNetV3-SSD-Lite</p>
    <p>&nbsp;&nbsp; 4-1. MobileNetV3-SSD-Lite を動かす（JPEGファイル）</p>
    <p>&nbsp;&nbsp; 4-2. MobileNetV3-SSD-Lite を動かす（PC 内蔵カメラ）</p>
    <p>&nbsp;&nbsp; 4-3. MobileNetV3-SSD-Lite を動かす（i-PRO カメラ）</p>
    <p>&nbsp;</p>
	<p><a href="#5._学習">5. 学習</a></p>
    <p>&nbsp;&nbsp; <a href="#5-1._まずはやってみる_(open_images)">5-1. まずはやってみる (open_images)</a></p>
    <p>&nbsp;&nbsp; <a href="#5-2._独自の画像を学習してみる_(VOC)">5-2. 独自の画像を学習してみる (VOC)</a></p>
    <br>
    <p><a href="#ライセンス">ライセンス</a></p>
    <p><a href="#参考">参考</a></p>
  </nav>
</div>
<p> &nbsp;</p>
<hr>
<p>&nbsp;</p>
<section>
  <p class="auto-style2"><strong>"i-PRO mini" 紹介： </strong></p>
  <ul>
	<li><a href="https://cwc.i-pro.com/pages/i-pro-mini-lp" target="_blank">
	i-PRO mini</a></li>
	<li>
	<a href="https://cwc.i-pro.com/collections/camera/products/wv-s7130ux" target="_blank">
	i-PRO mini 有線LANモデル WV-S7130UX</a></li>
	<li>
	<a href="https://cwc.i-pro.com/collections/camera/products/wv-s7130wux" target="_blank">
	i-PRO mini 無線LANモデル WV-S7130WUX</a></li>
	<li>
	<a href="https://japancs.i-pro.com/space/DLJP/724085590/WV-S7130UX　i-PRO+mini+有線LANモデル" target="_blank">
	WV-S7130UX　i-PRO mini 有線LANモデル - ダウンロード - i-PRO サポートポータル</a></li>
	<li>
	<a href="https://japancs.i-pro.com/space/DLJP/724086255/WV-S7130WUX　i-PRO+mini+無線LANモデル" target="_blank">
	WV-S7130WUX　i-PRO mini 無線LANモデル - ダウンロード - i-PRO サポートポータル</a></li>
  </ul>
  <p>
  <a href="images/i-PRO_mini.jpg" target="_blank">
  <img alt="i-PRO mini 画像" class="border_with_drop-shadow" src="images/i-PRO_mini.jpg" width="348"></a></p>
  <p>&nbsp;</p>
  <p class="auto-style2"><strong>"モジュールカメラ" 紹介：</strong></p>
  <ul>
	<li><a href="https://moduca.i-pro.com" target="_blank">モジュールカメラ｜ポータルサイト 
	(i-pro.com)</a></li>
	<li>
	<a href="https://moduca.i-pro.com/space/MCT/768743132/各種マニュアル" target="_blank">
	各種マニュアル - Module Camera Technical Information - モジュールカメラ｜ポータルサイト (i-pro.com)</a></li>
  </ul>
  <p>
  <a href="images/ai_starter_kit_1.png" target="_blank">
  <img alt="AIスターターキット画像（その１）" class="border_with_drop-shadow" src="images/ai_starter_kit_1.png" width="404"></a>
  <a href="images/ai_starter_kit_2.png" target="_blank">
  <img alt="AIスターターキット画像（その２）" class="border_with_drop-shadow" src="images/ai_starter_kit_2.png" width="444"></a></p>
  <p>&nbsp;</p>
  <p>カメラ初期設定についてはカメラ毎の取扱説明書をご確認ください。</p>
  <p>カメラのIPアドレスを確認・設定できる下記ツールを事前に入手しておくと便利です。</p>
  <ul>
	<li>
	<a href="https://connect.panasonic.com/jp-ja/products-services_security_support_specifications-manuals-firms-tool_2014040315191048" target="_blank">
	IP簡単設定ソフトウェア</a>&nbsp;（日本国内）</li>
	<li>
	<a href="https://bizpartner.panasonic.net/public/file/ip-setting-software" target="_blank">
	IP Setting Software</a>&nbsp;&nbsp;&nbsp;&nbsp; （グローバル）</li>
  </ul>
</section>

<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<section>
	<h2> <a name="5._学習">5. 学習</a></h2>
	<h4>[概要]</h4>
	<p> あなたが用意した画像データを使って独自の MobileNet-SSD 学習データを作成できるようになることを試みます。</p>
	<p> &nbsp;</p>
	
	<h3><a name="5-1._まずはやってみる_(open_images)">5-1. まずはやってみる (open_images)</a></h3>
	<h4>[概要]</h4>
	<p>今回も <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
	pytorch-ssd</a> で紹介されているプログラムと内容を使って進めます。</p>
	<p>ここでは "<a href="https://github.com/qfgaohao/pytorch-ssd#retrain-on-open-images-dataset" target="_blank">Retrain 
	on Open Images Dataset</a>" で説明されている記述に沿って実際にやってみます。学習済みデータをもとに 銃 
	を認識するようにファインチューニングする、というサンプルのようです。Python 
	プログラムはもちろん、画像データおよびアノテーションデータなど必要なすべての情報を提供いただいているので、最初に取り組みするには最適な内容と思われます。</p>
	<p>&nbsp;</p>
	  <p>使用する学習データ数は以下の通りです。</p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table.] 使用した学習データ数（重複画像を削除後の数です）</caption>
      <thead class="standard_table">
        <tr>
          <th>Class name</th>
          <th width="25%">Train</th>
          <th width="25%">Test</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Hundgun</strong></td>
          <td>545</td>
          <td>68</td>
        </tr>
        <tr>
          <td><strong>Shotgun</strong></td>
          <td>416</td>
          <td>55</td>
        </tr>
        <tr>
          <td><strong>合計</strong></td>
          <td>961</td>
          <td>123</td>
        </tr>
      </tbody>
    </table>
    
    <p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>"Hundgun" と "Shutgun" ２クラスの物体検知を行う、という内容です。</p>
      </div>
    </div>

    <p>&nbsp;</p>
	<p>&nbsp;</p>
	
	<section>
	<h4>5-1-1. Windows の場合</h4>
	<p>Windows 環境でどこまでできるか不安ですが、作業記録を残しつつ、できるところまで実際にやっていきたいと思います。</p>
	<p>&nbsp;</p>
	<h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.7</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.12.1+cpu</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>22H2</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
    
	<p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
	pytorch-ssd</a> をクローンします。（ここまで読まれた方は恐らく完了しているでしょう）</p>
	<p><a href="mobilenet-ssd.html#2-1._MobileNetV1-SSD_を動かす（PC 内蔵カメラ）">こちら</a> の記載を参考に実施します。</p>
	<p>&nbsp;</p>
	<p>2. 以下のコマンドで必要なライブラリーをインストールします。これらは後述の "open_images_downloader.py" 
	で使用しているライブラリです。</p>
	<p><span class="cpp-source">pip install <strong>boto3 pandas</strong></span></p>
	<p>&nbsp;</p>
	<p>3. ディレクトリ移動</p>
	<p>ターミナルソフトを起動後、 pytorch-ssd をクローンしたフォルダへ移動（"cd pytorch-ssd" など）します。</p>
	<p>または Explorer で目的フォルダを開いた後、Explorer のアドレスエリアで "cmd" + [Enter] します。</p>
	<p>&nbsp;</p>
	<p>4. まずは再学習済みデータを使ってデモ動作してみます。</p>
	<p><a href="https://github.com/qfgaohao/pytorch-ssd#retrain-on-open-images-dataset">"Retrain on Open Images Dataset"</a> 
	の説明では最初に以下の通り記載されています。</p>
	  
	<pre style="background-color: #000000; color: #CCCCCC; overflow-x: auto;">wget -P models https://storage.googleapis.com/models-hao/gun_model_2.21.pth
wget -P models https://storage.googleapis.com/models-hao/open-images-model-labels.txt
python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG</pre>

	<p>&nbsp;</p>
	<p>Windows環境では wget を標準で使用できません。代わりに <strong>bitsadmin.exe</strong> 
	というコマンドを使用することで下記のような感じで同じ内容を実行することができます。<br>
	<span class="auto-style3">c:\{作業フォルダ}</span> の部分をご自身の環境に合わせて修正して実行してください。</p>
	
  <pre style="background-color: #000000; color: #CCCCCC; overflow-x: auto;">bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/gun_model_2.21.pth <span class="auto-style3">c:\{作業フォルダ}</span>\models\gun_model_2.21.pth
bitsadmin /TRANSFER htmldl https://storage.googleapis.com/models-hao/open-images-model-labels.txt <span class="auto-style3">c:\{作業フォルダ}</span>\models\open-images-model-labels.txt
python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt <span class="auto-style3">{テストするJPEGファイル}</span></pre>

	<p>※ 記載の手順で進めても "big.JPG" ファイルはありませんでした。上記コマンドのテストに GUN のテストデータを必要としますので、ご自身で評価画像を入手するか、もしくは後述の「5. 
	  Download data」を実施後にこの画像ファイルを使って実験する、などする必要があります。</p>
	<p>&nbsp;</p>
	<p>5. Download data</p>
	<p>インターネットから学習データ一式を取得します。</p>
	<p>
	<a href="https://github.com/qfgaohao/pytorch-ssd#download-data" target="_blank">"Download data"</a>では下記のように書いていますが、Windowsではうまくいかず、エラーになりました。</p>
	
	<pre style="background-color: #000000; color: #CCCCCC; overflow-x: auto;">python open_images_downloader.py --root ~/data/open_images --class_names "Handgun,Shotgun" --num_workers 20</pre>
	
	<p>"--root" 
	の指定フォルダを下記のようにすることで無事ダウンロードすることができました。JPEG画像がそれなりの枚数あるので、私の環境で全データのダウンロードに５分ぐらいかかりました。</p>
	
	<pre style="background-color: #000000; color: #CCCCCC; overflow-x: auto;">python open_images_downloader.py --root <strong>./data/open_images</strong> --class_names "Handgun,Shotgun" --num_workers 20</pre>
	
	<p>&nbsp;</p>
	<p>6. 確認</p>
	<p>ダウンロード完了後の様子を下図に示します。</p>
	<p>指定したフォルダ "./data/open_images" の中に７つの csv ファイルと、３つのフォルダ（test, train, 
	validation）に多くのJPEG画像ファイルを保存していることがわかります。</p>
	<p>
	<img alt="ダウンロード後のフォルダの様子" src="mobilenet-ssd_train/imgA.jpg" width="800"></p>
	<p>&nbsp;</p>
	<p>"test" フォルダ内の様子です。</p>
	<p><img alt="test フォルダ内の画像(例)" src="mobilenet-ssd_train/imgF.jpg" width="800"></p>
	<p>&nbsp;</p>
	<p>"class-descriptions-boxable.csv" の様子です。分類するクラス一覧を記述しているようです。</p>
	  <p>"Handgun", "Shot gun" も含んでいました。私がダウンロードしたデータでは 601 種類のクラスを記録していました。</p>
	  <p>先ほどの手順では "Handgun,Shotgun" 
	  を指定してデータをダウンロードしましたが、こちらに記録されているクラスを指定して画像およびアノテーションデータをダウンロードできそうです。</p>
	<p><a href="mobilenet-ssd/img11.jpg" target="_blank">
	<img alt="class-descriptions-boxable.csv" class="auto-style6" src="mobilenet-ssd_train/img11.jpg" width="800"></a></p>
	<p>&nbsp;</p>
	<p>"sub-test-annotations-bbox.csv" の様子です。画像データおよびアノテーションデータ一覧のようです。</p>
	<p>XMin,XMax,YMin,YMax の４つは、画像データ上の物体の位置を示しています。それぞれ 0.0～1.0 
	の範囲で表記するルールとなっているため画像の解像度に影響されません。</p>
	<p>LabelName, id, に記載の情報は、ClassName 
	と紐づけられている情報のようです。"class-descriptions-boxable.csv" で例えば "/m/0gxl3" を検索すると 
	"Handgun" となっています。</p>
	<p><a href="mobilenet-ssd/img12.jpg" target="_blank">
	<img alt="sub-test-annotations-bbox.csv" class="auto-style6" src="mobilenet-ssd_train/img12.jpg" width="800"></a></p>
	<p>&nbsp;</p>
	<p>以上のような構成でデータを準備することで、あたなや私が独自に学習したい物体と画像についても同様に AI 学習データを作成できることがわかりました。</p>
	<p>&nbsp;</p>
	<p>7. 学習済みモデルを保存</p>
	<p>本ページを上から順に進めてきた人たちは既に models フォルダに学習済みモデルデータを保存済みと思います。まだの方は、<a href="#2-1._MobileNetV1-SSD_を動かす（PC 内蔵カメラ）">こちら</a>に記載の内容に従って 
	"mobilenet-v1-ssd-mp-0_675.pth" をダウンロードしておきます。</p>
	<p>&nbsp;</p>
	<p>8. Retrain (再学習)を実行します</p>
	<p>
	<a href="https://github.com/qfgaohao/pytorch-ssd#retrain" target="_blank">資料</a>では下記のように書いています。</p>
	
	<pre style="background-color: #000000; color: #CCCCCC; overflow-x: auto;">python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001  --batch_size 5</pre>
	
	<p>Windows 環境で動作する場合は少なくとも <span class="cpp-source">--datasets 
	~/data/open_images</span> 
	の部分を実際の環境に合わせて修正したほうがよさそうです。ここでは下記コマンドへ修正して実行してみます。</p>
	
	<pre style="background-color: #000000; color: #CCCCCC; overflow-x: auto;">python train_ssd.py --dataset_type open_images --datasets <strong>./data/open_images</strong> --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001  --batch_size 5</pre>
	
	<p>&nbsp;</p>
	
	<p>以下、上記コマンド実行後のコンソール出力内容です。</p>
	<p>データ読み込みまでは問題なくできていそうですが、"Start training from epoch 0." の後でエラーとなりました。</p>
	<pre style="background-color: #000000; color: #CCCCCC; overflow-x: auto;">2022-11-13 15:04:07,989 - root - INFO - Namespace(dataset_type='open_images', datasets=['./data/open_images'], validation_dataset=None, balance_data=False, net='mb1-ssd', freeze_base_net=False, freeze_net=False, mb2_width_mult=1.0, lr=0.01, momentum=0.9, weight_decay=0.0005, gamma=0.1, base_net_lr=0.001, extra_layers_lr=None, base_net=None, pretrained_ssd='models/mobilenet-v1-ssd-mp-0_675.pth', resume=None, scheduler='cosine', milestones='80,100', t_max=100.0, batch_size=5, num_epochs=100, num_workers=4, validation_epochs=5, debug_steps=100, use_cuda=True, checkpoint_folder='models/')
2022-11-13 15:04:07,989 - root - INFO - Prepare training datasets.
2022-11-13 15:04:08,277 - root - INFO - Dataset Summary:Number of Images: 961
Minimum Number of Images for a Class: -1
Label Distribution:
	Handgun: 727
	Shotgun: 580
2022-11-13 15:04:08,282 - root - INFO - Stored labels into file models/open-images-model-labels.txt.
2022-11-13 15:04:08,282 - root - INFO - Train dataset size: 961
2022-11-13 15:04:08,282 - root - INFO - Prepare Validation datasets.
2022-11-13 15:04:08,302 - root - INFO - Dataset Summary:Number of Images: 123
Minimum Number of Images for a Class: -1
Label Distribution:
	Handgun: 81
	Shotgun: 66
2022-11-13 15:04:08,302 - root - INFO - validation dataset size: 123
2022-11-13 15:04:08,302 - root - INFO - Build network.
2022-11-13 15:04:08,349 - root - INFO - Init from pretrained ssd models/mobilenet-v1-ssd-mp-0_675.pth
2022-11-13 15:04:08,391 - root - INFO - Took 0.04 seconds to load the model.
2022-11-13 15:04:08,396 - root - INFO - Learning rate: 0.01, Base net learning rate: 0.001, Extra Layers learning rate: 0.01.
2022-11-13 15:04:08,396 - root - INFO - Uses CosineAnnealingLR scheduler.
2022-11-13 15:04:08,396 - root - INFO - Start training from epoch 0.

C:\Users\foo\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\optim\lr_scheduler.py:131: <span class="auto-style3"><strong>UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "</strong></span>
Traceback (most recent call last):
  File "C:\Users\kinos\Documents\Github\pytorch-ssd\train_ssd.py", line 325, in &lt;module&gt;
    train(train_loader, net, criterion, optimizer,
  File "C:\Users\kinos\Documents\Github\pytorch-ssd\train_ssd.py", line 116, in train
    for i, data in enumerate(loader):
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 444, in __iter__
    return self._get_iterator()
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 390, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 1077, in __init__
    w.start()
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\context.py", line 336, in _Popen
    return Popen(process_obj)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\popen_spawn_win32.py", line 93, in __init__
    reduction.dump(process_obj, to_child)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
<span class="auto-style3"><strong>AttributeError: Can't pickle local object 'TrainAugmentation.__init__.&lt;locals&gt;.&lt;lambda&gt;'</strong></span>

C:\Users\kinos\Documents\Github\pytorch-ssd&gt;Traceback (most recent call last):
  File "&lt;string&gt;", line 1, in &lt;module&gt;
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\spawn.py", line 126, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input</pre>
	<p>&nbsp;</p>
	<p>"UserWarning: Detected call of `lr_scheduler.step()` before 
	`optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the 
	opposite order: `optimizer.step()` before `lr_scheduler.step()`.&nbsp; 
	Failure to do this will result in PyTorch skipping the first value of the 
	learning rate schedule. See more details at 
	https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate<br>&nbsp; 
	warnings.warn("Detected call of `lr_scheduler.step()` before 
	`optimizer.step()`." について</p>
	<p>こちらは、上記文章中に記載の通り "PyTorch 1.1.0" 以降での仕様変更に伴う警告のようです。</p>
	<p>ソースファイル "train_ssd.py" 中を 323行目 周辺について、下記のとおり変更することで警告されなくなります。</p>
	<p>（修正後）</p>
	<pre class="prettyprint linenums:323 lang-py">
    for epoch in range(last_epoch + 1, args.num_epochs):
        train(train_loader, net, criterion, optimizer,
              device=DEVICE, debug_steps=args.debug_steps, epoch=epoch)
<span class="auto-style1">        scheduler.step()</span>
        
        if epoch % args.validation_epochs == 0 or epoch == args.num_epochs - 1:
            val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)
            logging.info(
                f"Epoch: {epoch}, " +
                f"Validation Loss: {val_loss:.4f}, " +
                f"Validation Regression Loss {val_regression_loss:.4f}, " +
                f"Validation Classification Loss: {val_classification_loss:.4f}"
            )
            model_path = os.path.join(args.checkpoint_folder, f"{args.net}-Epoch-{epoch}-Loss-{val_loss}.pth")
            net.save(model_path)
            logging.info(f"Saved model {model_path}")
  </pre>
	<p>&nbsp;</p>
	<p>（修正前）</p>
	<pre class="prettyprint linenums:323 lang-py">
    for epoch in range(last_epoch + 1, args.num_epochs):
<span class="auto-style1">        scheduler.step()</span>
        train(train_loader, net, criterion, optimizer,
              device=DEVICE, debug_steps=args.debug_steps, epoch=epoch)
        
        if epoch % args.validation_epochs == 0 or epoch == args.num_epochs - 1:
            val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)
            logging.info(
                f"Epoch: {epoch}, " +
                f"Validation Loss: {val_loss:.4f}, " +
                f"Validation Regression Loss {val_regression_loss:.4f}, " +
                f"Validation Classification Loss: {val_classification_loss:.4f}"
            )
            model_path = os.path.join(args.checkpoint_folder, f"{args.net}-Epoch-{epoch}-Loss-{val_loss}.pth")
            net.save(model_path)
            logging.info(f"Saved model {model_path}")
  </pre>
	<p>&nbsp;</p>
	<p>（修正後）のプログラムを再度実行すると、とりあえず１つ目の警告（UserWarning）は消えました。</p>
	<pre style="background-color: #000000; color: #CCCCCC; overflow-x: auto;">C:\Users\foo\Documents\Github\pytorch-ssd&gt;python train_ssd.py --dataset_type open_images --datasets ./data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001  --batch_size 5
2022-11-13 16:35:24,380 - root - INFO - Namespace(dataset_type='open_images', datasets=['./data/open_images'], validation_dataset=None, balance_data=False, net='mb1-ssd', freeze_base_net=False, freeze_net=False, mb2_width_mult=1.0, lr=0.01, momentum=0.9, weight_decay=0.0005, gamma=0.1, base_net_lr=0.001, extra_layers_lr=None, base_net=None, pretrained_ssd='models/mobilenet-v1-ssd-mp-0_675.pth', resume=None, scheduler='cosine', milestones='80,100', t_max=100.0, batch_size=5, num_epochs=100, num_workers=4, validation_epochs=5, debug_steps=100, use_cuda=True, checkpoint_folder='models/')
2022-11-13 16:35:24,380 - root - INFO - Prepare training datasets.
2022-11-13 16:35:24,662 - root - INFO - Dataset Summary:Number of Images: 961
Minimum Number of Images for a Class: -1
Label Distribution:
        Handgun: 727
        Shotgun: 580
2022-11-13 16:35:24,662 - root - INFO - Stored labels into file models/open-images-model-labels.txt.
2022-11-13 16:35:24,662 - root - INFO - Train dataset size: 961
2022-11-13 16:35:24,662 - root - INFO - Prepare Validation datasets.
2022-11-13 16:35:24,693 - root - INFO - Dataset Summary:Number of Images: 123
Minimum Number of Images for a Class: -1
Label Distribution:
        Handgun: 81
        Shotgun: 66
2022-11-13 16:35:24,693 - root - INFO - validation dataset size: 123
2022-11-13 16:35:24,693 - root - INFO - Build network.
2022-11-13 16:35:24,743 - root - INFO - Init from pretrained ssd models/mobilenet-v1-ssd-mp-0_675.pth
2022-11-13 16:35:24,774 - root - INFO - Took 0.03 seconds to load the model.
2022-11-13 16:35:24,789 - root - INFO - Learning rate: 0.01, Base net learning rate: 0.001, Extra Layers learning rate: 0.01.
2022-11-13 16:35:24,789 - root - INFO - Uses CosineAnnealingLR scheduler.
2022-11-13 16:35:24,789 - root - INFO - Start training from epoch 0.
Traceback (most recent call last):
  File "C:\Users\kinos\Documents\Github\pytorch-ssd\train_ssd.py", line 324, in &lt;module&gt;
    train(train_loader, net, criterion, optimizer,
  File "C:\Users\kinos\Documents\Github\pytorch-ssd\train_ssd.py", line 116, in train
    for i, data in enumerate(loader):
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 444, in __iter__
    return self._get_iterator()
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 390, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py", line 1077, in __init__
    w.start()
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\process.py", line 121, in start
    self._popen = self._Popen(self)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\context.py", line 336, in _Popen
    return Popen(process_obj)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\popen_spawn_win32.py", line 93, in __init__
    reduction.dump(process_obj, to_child)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
<span class="auto-style3"><strong>AttributeError: Can't pickle local object 'TrainAugmentation.__init__.&lt;locals&gt;.&lt;lambda&gt;'</strong></span>

C:\Users\kinos\Documents\Github\pytorch-ssd&gt;Traceback (most recent call last):
  File "&lt;string&gt;", line 1, in &lt;module&gt;
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "C:\Users\kinos\AppData\Local\Programs\Python\Python310\lib\multiprocessing\spawn.py", line 126, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input</pre>
	<p>&nbsp;</p>
	<p>次に残るエラーを解決します。</p>
	<p>この問題を解決している記事がありました。</p>
	<p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>上記問題のうち 「AttributeError: Can't pickle local object 
		'TrainAugmentation.__init__.&lt;locals&gt;.&lt;lambda&gt;'」 については下記ページで議論されてそうです。</p>
		<p>
		<a href="https://github.com/qfgaohao/pytorch-ssd/issues/71" target="_blank">
		AttributeError: Can't pickle local object 
		'TrainAugmentation.__init__.&lt;locals&gt;.&lt;lambda&gt;' · Issue #71 · 
		qfgaohao/pytorch-ssd (github.com)</a></p>
      </div>
    </div>

    <p>&nbsp;</p>
	<p>上記URLに記載の内容に従って修正した後の「vision/ssd/data_preprocessing.py」を以下に記載します。<br>
	修正した場所を色付けしています。</p>
	<p>&nbsp;</p>
	<p>"vision/ssd/data_preprocessing.py"</p>
	<pre class="prettyprint linenums lang-py">from ..transforms.transforms import *


<span class="auto-style1">class ScaleByStd:</span>
<span class="auto-style1">    def __init__(self, std: float):</span>
<span class="auto-style1">        self.std = std</span>

<span class="auto-style1">    def __call__(self, img, boxes=None, labels=None):</span>
<span class="auto-style1">        return (img / self.std, boxes, labels)</span>


class TrainAugmentation:
    def __init__(self, size, mean=0, std=1.0):
        """
        Args:
            size: the size the of final image.
            mean: mean pixel value per channel.
        """
        self.mean = mean
        self.size = size
        self.augment = Compose([
            ConvertFromInts(),
            PhotometricDistort(),
            Expand(self.mean),
            RandomSampleCrop(),
            RandomMirror(),
            ToPercentCoords(),
            Resize(self.size),
            SubtractMeans(self.mean),
<span class="auto-style1">            #lambda img, boxes=None, labels=None: (img / std, boxes, labels),</span>
<span class="auto-style1">            ScaleByStd(std),</span>
            ToTensor(),
        ])

    def __call__(self, img, boxes, labels):
        """

        Args:
            img: the output of cv.imread in RGB layout.
            boxes: boundding boxes in the form of (x1, y1, x2, y2).
            labels: labels of boxes.
        """
        return self.augment(img, boxes, labels)


class TestTransform:
    def __init__(self, size, mean=0.0, std=1.0):
        self.transform = Compose([
            ToPercentCoords(),
            Resize(size),
            SubtractMeans(mean),
<span class="auto-style1">            #lambda img, boxes=None, labels=None: (img / std, boxes, labels),</span>
<span class="auto-style1">            ScaleByStd(std),</span>
            ToTensor(),
        ])

    def __call__(self, image, boxes, labels):
        return self.transform(image, boxes, labels)


class PredictionTransform:
    def __init__(self, size, mean=0.0, std=1.0):
        self.transform = Compose([
            Resize(size),
            SubtractMeans(mean),
<span class="auto-style1">            #lambda img, boxes=None, labels=None: (img / std, boxes, labels),</span>
<span class="auto-style1">            ScaleByStd(std),</span>
            ToTensor()
        ])

    def __call__(self, image):
        image, _, _ = self.transform(image)
        return image</pre>
	<p>&nbsp;</p>
	<p>上記修正を行うことで、下記コマンドを正常に実行できるようになりました。</p>
	
	<pre style="background-color: #000000; color: #CCCCCC; overflow-x: auto;">python train_ssd.py --dataset_type open_images --datasets <strong>./data/open_images</strong> --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001  --batch_size 5</pre>
	
	<p>&nbsp;</p>
	<p><span class="auto-style2"><strong>100 Epoch を約５時間半で学習できました</strong></span>。（CPU動作の場合です。GPU動作の場合はもっと高速に学習可能です。）</p>
	<p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
		    <p>train_ssd.py を正常に実行できるようにはなりましたが、私の環境では下記警告？が頻繁に出力されます。</p>
        <p><span class="cpp-source">pytorch-ssd\vision\transforms\transforms.py:247: 
		VisibleDeprecationWarning: Creating an ndarray from ragged nested 
		sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with 
		different lengths or shapes) is deprecated. If you meant to do this, you 
		must specify 'dtype=object' when creating the ndarray.<br>&nbsp; mode = 
		random.choice(self.sample_options)</span></p>
		<p>このままでも正常に動作するようですが、修正したい場合は下記URLの記事を参照。</p>
		<p>
		<a href="https://github.com/amdegroot/ssd.pytorch/issues/498" target="_blank">VisibleDeprecationWarning of augmentations.py · Issue #498 · 
		amdegroot/ssd.pytorch (github.com)</a></p>
      </div>
    </div>

    <p>&nbsp;</p>
	<p>"transforms.py" の 247行目を以下のように修正すればよいようです。</p>
	<pre class="prettyprint linenums lang-py" style="width: 800px; overflow-x:auto;">
# before
mode = random.choice(self.sample_options)

# after
random_idx = random.randint(0, len(self.sample_options) - 1)
mode = self.sample_options[random_idx]</pre>
	<p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>"models\open-images-model-labels.txt" は "train_ssd.py" 
		を実行することで自動的に生成されるようにプログラミングされているようです。</p>
        <p>dataset_type == 'voc' の場合は "voc-model-labels.txt" を自動的に生成するようです。</p>
		<p>２つの生成方法は細かい部分でいろいろと差異がありそうです。詳細については "train_ssd.py" を参照してください。</p>
      </div>
    </div>

    <p>&nbsp;</p>
	  <p>&nbsp;</p>
  	<p>9. 学習したデータとテスト画像を使用して Gun の認識をしてみます。</p>
	  <p>テスト画像は <a href="https://pixabay.com/ja/" target="_blank">Pixabay</a> 
    から入手した画像を使用します。</p>
    <p>&nbsp;</p>
    <p>私の学習済みデータ（"mb1-ssd-Epoch-99-Loss-2.843603060245514.pth"）とサンプル画像（"sample_image.jpg"）を使った場合の例を以下に記載します。あなたが実際に使用するファイル名およびパスへ修正してください。</p>
    
    <pre style="background-color: #000000; color: #CCCCCC; overflow-x: auto;">python run_ssd_example.py mb1-ssd .\models\mb1-ssd-Epoch-99-Loss-2.843603060245514.pth models\open-images-model-labels.txt .\sample_image.jpg</pre>
    
    <p>&nbsp;</p>
    <p>ただし <strong>run_ssd_example.py</strong> は OpenSSL 
  	のバージョンアップに伴って一部修正する必要があります。修正後のソースコードを下記に示します。<br>60, 64行目の部分で６か所 int 
  	キャストを追加する必要があります。この修正を行わないとプログラム実行時にエラーとなって動作しませんでした。</p>
    <p>&nbsp;</p>
    <p>"run_ssd_example.py"</p>
	  <pre class="prettyprint linenums lang-py">from vision.ssd.vgg_ssd import create_vgg_ssd, create_vgg_ssd_predictor
from vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd, create_mobilenetv1_ssd_predictor
from vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite, create_mobilenetv1_ssd_lite_predictor
from vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite, create_squeezenet_ssd_lite_predictor
from vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite, create_mobilenetv2_ssd_lite_predictor
from vision.ssd.mobilenetv3_ssd_lite import create_mobilenetv3_large_ssd_lite, create_mobilenetv3_small_ssd_lite
from vision.utils.misc import Timer
import cv2
import sys


if len(sys.argv) &lt; 5:
    print('Usage: python run_ssd_example.py &lt;net type&gt;  &lt;model path&gt; &lt;label path&gt; &lt;image path&gt;')
    sys.exit(0)
net_type = sys.argv[1]
model_path = sys.argv[2]
label_path = sys.argv[3]
image_path = sys.argv[4]

class_names = [name.strip() for name in open(label_path).readlines()]

if net_type == 'vgg16-ssd':
    net = create_vgg_ssd(len(class_names), is_test=True)
elif net_type == 'mb1-ssd':
    net = create_mobilenetv1_ssd(len(class_names), is_test=True)
elif net_type == 'mb1-ssd-lite':
    net = create_mobilenetv1_ssd_lite(len(class_names), is_test=True)
elif net_type == 'mb2-ssd-lite':
    net = create_mobilenetv2_ssd_lite(len(class_names), is_test=True)
elif net_type == 'mb3-large-ssd-lite':
    net = create_mobilenetv3_large_ssd_lite(len(class_names), is_test=True)
elif net_type == 'mb3-small-ssd-lite':
    net = create_mobilenetv3_small_ssd_lite(len(class_names), is_test=True)
elif net_type == 'sq-ssd-lite':
    net = create_squeezenet_ssd_lite(len(class_names), is_test=True)
else:
    print("The net type is wrong. It should be one of vgg16-ssd, mb1-ssd and mb1-ssd-lite.")
    sys.exit(1)
net.load(model_path)

if net_type == 'vgg16-ssd':
    predictor = create_vgg_ssd_predictor(net, candidate_size=200)
elif net_type == 'mb1-ssd':
    predictor = create_mobilenetv1_ssd_predictor(net, candidate_size=200)
elif net_type == 'mb1-ssd-lite':
    predictor = create_mobilenetv1_ssd_lite_predictor(net, candidate_size=200)
elif net_type == 'mb2-ssd-lite' or net_type == "mb3-large-ssd-lite" or net_type == "mb3-small-ssd-lite":
    predictor = create_mobilenetv2_ssd_lite_predictor(net, candidate_size=200)
elif net_type == 'sq-ssd-lite':
    predictor = create_squeezenet_ssd_lite_predictor(net, candidate_size=200)
else:
    predictor = create_vgg_ssd_predictor(net, candidate_size=200)

orig_image = cv2.imread(image_path)
image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)
boxes, labels, probs = predictor.predict(image, 10, 0.4)

for i in range(boxes.size(0)):
    box = boxes[i, :]
    cv2.rectangle(orig_image, (<span class="auto-style1">int(box[0])</span>, <span class="auto-style1">int(box[1])</span>), (<span class="auto-style1">int(box[2])</span>, <span class="auto-style1">int(box[3])</span>), (255, 255, 0), 4)
    #label = f"""{voc_dataset.class_names[labels[i]]}: {probs[i]:.2f}"""
    label = f"{class_names[labels[i]]}: {probs[i]:.2f}"
    cv2.putText(orig_image, label,
                (<span class="auto-style1">int(box[0])</span> + 20, <span class="auto-style1">int(box[1])</span> + 40),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,  # font scale
                (255, 0, 255),
                2)  # line type
path = "run_ssd_example_output.jpg"
cv2.imwrite(path, orig_image)
print(f"Found {len(probs)} objects. The output image is {path}")</pre>
	<p>&nbsp;</p>
	  <p>以下、テストした結果をいくつか示します。</p>
	  <p>&nbsp;</p>
	  <p>テスト画像１： 入手元 
	  <a href="https://pixabay.com/ja/photos/ハーレイ-クイン-コスプレ-漫画-5391062/" target="_blank">
	  Pixabay</a></p>
	  <p>
	  <span style="color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;">
	  商用利用無料、帰属表示必要なし、の画像です。ライセンス詳細は "<a href="https://pixabay.com/ja/service/terms/" target="_blank">Pixabay 
	  License</a>" を参照ください。</span></p>
	  <p>["harley-quinn-5391062_1920.jpg"]</p>
	  <p>
	  <img alt="harley-quinn-5391062_1920.jpg" src="mobilenet-ssd_train/imgA1.jpg" width="800"></p>
	  <p>&nbsp;</p>
	  <p>テスト画像２： 入手元 
	  <a href="https://pixabay.com/ja/photos/銃-フランスの外legion-4222469/" target="_blank">
	  Pixabay</a></p>
	  <p>
	  <span style="color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;">
	  商用利用無料、帰属表示必要なし、の画像です。ライセンス詳細は "<a href="https://pixabay.com/ja/service/terms/" target="_blank">Pixabay 
	  License</a>" を参照ください。</span></p>
	  <p>["gun-4222469_1920.jpg"]</p>
	  <p>
	  <img alt="gun-4222469_1920.jpg" src="mobilenet-ssd_train/img4.jpg" width="800"></p>
	  <p>&nbsp;</p>
	  <p>テスト画像３： 入手元 
	  <a href="https://pixabay.com/ja/photos/軍-武器-アフガニ-反逆者-60720/" target="_blank">
	  Pixabay</a></p>
	  <p>
	  <span style="color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;">
	  商用利用無料、帰属表示必要なし、の画像です。ライセンス詳細は "<a href="https://pixabay.com/ja/service/terms/" target="_blank">Pixabay 
	  License</a>" を参照ください。</span></p>
	  <p>["army-60720_1920.jpg"]</p>
	  <p>
	  <img alt="army-60720_1920.jpg" src="mobilenet-ssd_train/img7.jpg" width="800"></p>
	  <p>&nbsp;</p>
	  <p>&nbsp;</p>
	  <p>10. "--freeze_net", "--freeze_base_net"</p>
	  <p>ホームページ中に下記説明がありました。</p>
	  <p>
  	<img alt="--freeze_base_net, --freeze_net" class="border_with_drop-shadow" src="mobilenet-ssd_train/img1B.jpg" width="800"></p>
	  <p>&nbsp;</p>
    <p><span class="cpp-source"><strong>--freeze_net</strong></span> を指定することで "prediction head" 
    を除いて全レイヤーをフリーズと記載されています。俗にいう "<strong>転移学習</strong>" 
    を行えそうです。これを指定することで演算が軽くなることを期待できます。</p>
    <p><span class="cpp-source"><strong>--freeze_base_net</strong></span> を指定することで "base net 
    layers" をフリーズするようです。こちらも "<strong>転移学習</strong>" のバリエーションの一つですね。本ページの例では、MobileNet 部分をフリーズして SSD or SSD-Lite 
    部分をフリーズしない、と読めば良いのかな、と想像しています。"--freeze_net" よりは学習の演算量が多いはずです。</p>
	  <p>&nbsp;</p>
	  <p>ここでは学習時間の短縮を目的に、引数 "<strong>--freeze_net</strong>", "<strong>--freeze_base_net</strong>" を試してみます。それぞれ 
	  100 Epoch 学習させたのちの "Validation Loss" についても比較してみます。</p>
	  <p>以下、学習時の引数を変更したときの学習時間、代表的な Epoch における "Validation Loss" の表です。<br>
	  AI学習は乱数を使っている部分もあるので、私と同じ手順を行っても同じ結果になりません。私自身が同じことを複数回行っても異なる結果になったりします。あくまで参考値ということで。</p>
	  <p>&nbsp;</p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table.] 引数毎の 学習時間、Validation Loss 比較</caption>
      <thead class="standard_table">
        <tr>
          <th style="white-space: nowrap">Epoch num.</th>
          <th width="25%" style="white-space: nowrap">Normal<br>(追加引数無し)</th>
          <th width="25%" style="white-space: nowrap">--freeze_net</th>
          <th width="25%" style="white-space: nowrap">--freeze_base_net</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>0</strong></td>
          <td>3.77</td>
          <td>8.19</td>
          <td>4.63</td>
        </tr>
        
        <tr>
          <td><strong>10</strong></td>
          <td>2.98</td>
          <td>7.98</td>
          <td>4.10</td>
        </tr>
        
        <tr>
          <td><strong>20</strong></td>
          <td>2.98</td>
          <td>8.02</td>
          <td>3.95</td>
        </tr>
        
        <tr>
          <td><strong>30</strong></td>
          <td>2.82</td>
          <td>6.27</td>
          <td>3.76</td>
        </tr>
        
        <tr>
          <td><strong>40</strong></td>
          <td>2.75</td>
          <td>5.91</td>
          <td>3.59</td>
        </tr>
        
        <tr>
          <td><strong>50</strong></td>
          <td>3.00</td>
          <td>4.92</td>
          <td>3.49</td>
        </tr>
        
        <tr>
          <td><strong>60</strong></td>
          <td>2.83</td>
          <td>4.45</td>
          <td>3.25</td>
        </tr>
        
        <tr>
          <td><strong>70</strong></td>
          <td>2.89</td>
          <td>3.83</td>
          <td>3.17</td>
        </tr>
        
        <tr>
          <td><strong>80</strong></td>
          <td>2.80</td>
          <td>3.58</td>
          <td>3.07</td>
        </tr>
        
        <tr>
          <td><strong>90</strong></td>
          <td>2.86</td>
          <td>3.42</td>
          <td>3.01</td>
        </tr>
        
        <tr>
          <td><strong>99</strong></td>
          <td>2.84</td>
          <td>3.40</td>
          <td>3.01</td>
        </tr>
        
        <tr>
          <td><strong>学習時間</strong></td>
          <td style="white-space: nowrap">5時間30分</td>
          <td style="white-space: nowrap">2時間20分</td>
          <td style="white-space: nowrap">2時間40分</td>
        </tr>
        
      </tbody>
    </table>
    
    <p>&nbsp;</p>
	  <p>"--freeze_net", "--freeze_base_net" の学習時間は半分以下になりました。認識結果は Normal 
	  より悪いですがそこそこ認識しています。</p>
	  <p>認識性能（Validation Loss）は Normal が最も良い結果となりました。</p>
	  <p>それぞれメリット／デメリットがありそうなので、用途などに応じて使い分けてみてはいかがでしょうか。</p>
	  <p>以下、サンプル画像による認識結果例です。</p>
	  <p>&nbsp;</p>
	  <p>["Normal" Epoch99 による認識結果例]</p>
	  <p>
	  <img alt="gun-4222469_1920.jpg" src="mobilenet-ssd_train/img4.jpg" width="800"></p>
	  <p>&nbsp;</p>
	  <p>["--freeze_net" Epoch99 による認識結果例]</p>
	  <p><img alt="--freeze_net" src="mobilenet-ssd_train/img13.jpg" width="800"></p>
	  <p>&nbsp;</p>
	  <p>["--freeze_base_net" Epoch99 による認識結果例]</p>
	  <p><img alt="--freeze_base_net" src="mobilenet-ssd_train/img14.jpg" width="800"></p>
	  <p>&nbsp;</p>
	  <p>&nbsp;</p>
	  <p>11. スケジューラを "multi-step" にしてみる</p>
	  <p>"multi-step" は学習速度の減速タイミングを詳細に指示することができます。</p>
	  <p>スケジューラを「cosine」とした上記学習では、Epoch 30～50 あたりから学習の停滞がみられます。"Validation Loss" の値だと 2.8～2.9 
	  ぐらいから良くなりません。この辺りに達した後、学習速度をさらに減速することでより良い結果を得られるのではという仮説から、スケジューラを「multi-step」へ変更、--milestones として "50,80" 
	  を指定して実験してみます。</p>
	  <p>引数 --milestones で指定した Epoch で学習速度を 0.1倍 へ減速します。下記例は Epoch 0-49 まで初期値、Epoch 50-79 を 0.1倍、Epoch 80-99 をさらに 
	  0.1倍、つまり初期値の 0.01倍へ減速します。</p>
	  
	  <pre style="background-color: #000000; color: #CCCCCC; overflow-x: auto;">python train_ssd.py --dataset_type open_images --datasets ./data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth <span class="auto-style3"><strong>--scheduler multi-step</strong> <strong>--milestones 50,80</strong></span> --lr 0.01 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001  --batch_size 5</pre>
	  
	  <p>&nbsp;</p>
	  <p>下記表へ Epoch で99までの学習の様子を記載します。１回のみの結果なので、複数回実施するとまた違う結果になるかもしれません。</p>
	  <p>&nbsp;</p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table.] 引数毎の 学習時間、Validation Loss 比較</caption>
      <thead class="standard_table">
        <tr>
          <th style="white-space: nowrap" rowspan="2">Epoch num.</th>
          <th width="25%" style="white-space: nowrap" class="auto-style7" colspan="2">Cosine</th>
          <th width="50%" style="white-space: nowrap" colspan="2">Multi-step (50, 80)</th>
        </tr>
        <tr>
          <th style="white-space: nowrap">Validation Loss</th>
          <th style="white-space: nowrap">Average Loss</th>
          <th style="white-space: nowrap">Validation Loss</th>
          <th style="white-space: nowrap">Average Loss</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>0</strong></td>
          <td>3.58</td>
          <td>5.95</td>
          <td>3.68</td>
          <td>5.86</td>
        </tr>
        
        <tr>
          <td><strong>10</strong></td>
          <td>3.07</td>
          <td>3.38</td>
          <td>3.10</td>
          <td>3.40</td>
        </tr>
        
        <tr>
          <td><strong>20</strong></td>
          <td>2.87</td>
          <td>3.08</td>
          <td>3.08</td>
          <td>3.10</td>
        </tr>
        
        <tr>
          <td><strong>30</strong></td>
          <td>2.77</td>
          <td>2.75</td>
          <td>2.81</td>
          <td>2.94</td>
        </tr>
        
        <tr>
          <td><strong>40</strong></td>
          <td>2.84</td>
          <td>2.52</td>
          <td>2.92</td>
          <td>2.91</td>
        </tr>
        
        <tr>
          <td><strong>50</strong></td>
          <td>2.83</td>
          <td>2.41</td>
          <td>2.72</td>
          <td>2.69</td>
        </tr>
        
        <tr>
          <td><strong>60</strong></td>
          <td>2.82</td>
          <td>2.27</td>
          <td>2.75</td>
          <td>2.27</td>
        </tr>
        
        <tr>
          <td><strong>70</strong></td>
          <td>2.77</td>
          <td>2.01</td>
          <td>2.72</td>
          <td>2.17</td>
        </tr>
        
        <tr>
          <td><strong>80</strong></td>
          <td>2.80</td>
          <td>1.99</td>
          <td>2.72</td>
          <td>2.10</td>
        </tr>
        
        <tr>
          <td><strong>90</strong></td>
          <td>2.78</td>
          <td>1.84</td>
          <td>2.75</td>
          <td>2.01</td>
        </tr>
        
        <tr>
          <td><strong>99</strong></td>
          <td>2.86</td>
          <td>1.69</td>
          <td><strong>2.72</strong></td>
          <td>1.99</td>
        </tr>
        
        <tr>
          <td><strong>学習時間</strong></td>
          <td style="white-space: nowrap" class="auto-style7" colspan="2">5時間30分</td>
          <td style="white-space: nowrap" colspan="2">5時間30分</td>
        </tr>
        
      </tbody>
    </table>
    
	  <p>&nbsp;</p>
	  <p>["multistep" Epoch99 による認識結果例]</p>
	  <p><img alt="multi-step" src="mobilenet-ssd_train/img15.jpg" width="800"></p>
	  <p>&nbsp;</p>
	  <p>少なくとも "multi-step" による学習はできてそうです。枠の位置はこれが最も良い結果に見えます。</p>
	  <p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>参考</strong></p>
        <p>ゲーミングノートＰＣ環境でも同じ学習を実施。<br>5時間30分 ⇨ <strong>55分</strong> へ短縮できました。</p>
    		<p>[環境]</p>
    		<p>GPU: NVIDIA GeForce GTX 1650 (4GB)</p>
    		<p>CPU: Intel Core i7-9750H</p>
    		<p>Cuda: 11.7</p>
    		<p>Python: 3.10.7</p>
    		<p>PyToch: 1.12+cu116</p>
		    <p>OS: Windows 11 home, 22H2</p>
      </div>
    </div>

	  <p>&nbsp;</p>
	</section>
	
	<p>&nbsp;</p>
	<hr>
	<p>&nbsp;</p>
	
	<section>
	<h4>5-1-2. Linux の場合</h4>
	<p>&nbsp;</p>
	<p>To Be Edited. </p>
	<p>&nbsp;</p>
</section>
	
<p>&nbsp;</p>
	<hr>
	<p>&nbsp;</p>

<section>
<h3><a name="5-2._独自の画像を学習してみる_(VOC)">5-2. 独自の画像を学習してみる (VOC)</a></h3>
	<p>&nbsp;</p>
  <p class="auto-style3">※ 記載途中です。</p>
  <p>&nbsp;</p>
  <h4>[概要]</h4>
  <p>今度は自身で集めた画像を使ってオリジナルの学習データによる学習を試みます。</p>
  <p>前節で使用した train_ssd.py は入力データ型として "open_images" と "voc" の２種類に対応しているようです。ここでは "voc" 
  の出力に対応していて有名なオープンソースツール 
  <a href="https://github.com/heartexlabs/labelImg" target="_blank">LabelImg</a> を使用してアノテーションデータを作成してみます。</p>
  <p>そして自身で実際に作成したアノテーションデータを使って学習を行ってみます。少なめのデータで技術的に実現できることまでを目標に進めてみたいと思います。認識性能を高くしたい場合は同じ要領で学習する画像データを増やせば良いので、本ページではあくまで手順の確認と説明まで行うことにします。</p>
  <p>&nbsp;</p>
	  <p>準備した画像データは以下の通りです。<a href="https://pixabay.com/ja" target="_blank">pixabay</a> 
	  から入手させていただきました。</p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table.] 使用した学習データ数</caption>
      <thead class="standard_table">
        <tr>
          <th>Class name</th>
          <th width="25%">TrainVal</th>
          <th width="25%">Test</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>bird</td>
          <td>140</td>
          <td>60</td>
        </tr>
      </tbody>
    </table>
    
      <p>&nbsp;</p>
	
	<h4>[環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.7</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.12.1+cpu</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>22H2</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

	<p> &nbsp;</p>
	
	<h4>[手順]</h4>
  <p>&nbsp;</p>
  <p>1. "train_ssd.py" が想定する VOC データセットのフォルダ・ファイルを準備</p>
  <p>voc 形式のフォルダおよびファイル構成は下図の通りです。この構成図に従ってフォルダを作成します。</p>
  <p>&nbsp;</p>
  <p>
  <img alt="フォルダ、ファイル構成" src="mobilenet-ssd_train/folder_struct.drawio.svg" width="500"></p>
  <p>&nbsp;</p>
        
    <table class="border-collapse" border="1" width="800">
      <caption>[Table.] フォルダおよびファイル構成</caption>
      <thead class="standard_table">
        <tr>
          <th>名称</th>
          <th>説明</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Annotations</td>
          <td>アノテーション結果を記録した XML ファイルを保存します。</td>
        </tr>
        <tr>
          <td>JPEGImages</td>
          <td>JPEG ファイルを保存します。</td>
        </tr>
        <tr>
          <td>ImageSets/Main/test.txt</td>
          <td>テスト用ファイルのファイル名一覧を記録します。拡張子なしで記載します。</td>
        </tr>
        <tr>
          <td>ImageSets/Main/trainval.txt</td>
          <td>学習用ファイルのファイル名一覧を記録します。拡張子なしで記載します。</td>
        </tr>
        <tr>
          <td>labels.txt</td>
          <td>ラベル一覧を記録済ます。</td>
        </tr>
      </tbody>
    </table>
    
    <p>&nbsp;</p>
  <p>&nbsp;</p>
  <p>2. 学習用の画像を集めて "JPEGImages" フォルダへ保存します。</p>
  <p>学習用160枚、テスト用40枚、合計200枚の JPEG ファイルをすべてここに保存しました。</p>
  <p>&nbsp;</p>
  <p>3. LabelImg をインストール</p>
  <p>アノテーションツール LabelImg をインストールします。pip, setuptools を更新した後、pip で labelimg 
  をインストールします。</p>
  <pre style="background-color: #000000; color: #CCCCCC; width: 800px;overflow-x: auto;">python -m pip install --upgrade pip setuptools
pip3 install labelImg
</pre>
  <p>&nbsp;</p>
  <p>4. LabelImg を起動、設定</p>
  <p>LabelImg の起動方法は下記いずれかで行います。</p>
  <p>[IMAGE_PATH]： JPEG 画像を保存しているパスを指定します。</p>
  <p>[PRE-DEFINED CLASS FILE]： ラベル名を保存しているテキストファイルのパスを指定します。LabelImg 
  でアノテーション時に画面でリストから選択するだけでクラスを選択することが可能になります。</p>
  <pre style="background-color: #000000; color: #CCCCCC; width: 800px;overflow-x: auto;">labelImg
labelImg [IMAGE_PATH] [PRE-DEFINED CLASS FILE]</pre>
  <p>&nbsp;</p>
  <p>5. LabelImg を使ってアノテーション実施</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
  <p>6. "ImageSets\Main\trainval.txt" を設定</p>
  <p>T.B.E.</p>
  <p>全画像データから 70% を学習用（trainval）に、30% を評価用（test）に使用してみます。</p>
  <p>&nbsp;</p>
  <p>7. "ImageSets\Main\test.txt" を設定</p>
  <p>T.B.E.</p>
  <p>全画像データから 70% を学習用（trainval）に、30% を評価用（test）に使用してみます。</p>
  <p>&nbsp;</p>
  <p>8. "labels.txt" を設定</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
  <p>9. 学習を実施</p>
  <p>T.B.E.</p>
  <p>&nbsp;</p>
</section>

<p>&nbsp;</p>
</section>

<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<section>
  <h2><a id="ライセンス">ライセンス</a></h2>
<p>本ページの情報は、特記無い限り下記 MIT ライセンスで提供されます。</p>
<div class="license">
The MIT License (MIT)<br><br>

Copyright © 2023 Kinoshita Hidetoshi<br><br>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:<br><br>

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.<br><br>

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</div>
  <p>&nbsp;</p>
</section>

<br>

<section>
	<h2><a id="参考">参考</a></h2>
	<ul>
		<li>[1] PyTorch<br><a href="https://pytorch.org/" target="_blank">
      https://pytorch.org/</a></li>
    <li>[2] qfgaohao/pytorch-ssd: MobileNetV1, MobileNetV2, VGG based 
      SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box 
      support for retraining on Open Images dataset. ONNX and Caffe2 support. 
      Experiment Ideas like CoordConv. (github.com)<br>
      <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
      https://github.com/qfgaohao/pytorch-ssd</a></li>
    <li>[3] PyTorchでMobileNet SSDによるリアルタイム物体検出｜はやぶさの技術ノート (cpp-learning.com)<br>
      <a href="https://cpp-learning.com/pytorch_mobilenet-ssd/" target="_blank">
      https://cpp-learning.com/pytorch_mobilenet-ssd/</a></li>
    <li>[4] MobilenetSSD : 高速に物体検出を行う機械学習モデル. ailia… | by Kazuki Kyakuno | axinc | Medium<br>
      <a href="https://medium.com/axinc/mobilenetssd-高速に物体検出を行う機械学習モデル-be3ca37c411" target="_blank">
      https://medium.com/axinc/mobilenetssd-高速に物体検出を行う機械学習モデル-be3ca37c411</a></li>
		<li>[5] Everything You Need To Know About Torchvision’s SSDlite Implementation | PyTorch<br>
      <a href="https://pytorch.org/blog/torchvision-ssdlite-implementation/" target="_blank">
      https://pytorch.org/blog/torchvision-ssdlite-implementation/</a></li>
		<li>[6] vision/ssdlite.py at b6f733046c9259f354d060cd808241a558d7d596 · pytorch/vision · GitHub<br>
  		<a href="https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L159-L162" target="_blank">
	  	https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L159-L162</a></li>
		<li>[7] Models and pre-trained weights — Torchvision main documentation (pytorch.org)<br>
		  <a href="https://pytorch.org/vision/main/models.html" target="_blank">
		  https://pytorch.org/vision/main/models.html</a></li>
		<li>[8] Visualization utilities — Torchvision 0.11.0 documentation (pytorch.org)<br>
      <a href="https://pytorch.org/vision/0.11/auto_examples/plot_visualization_utils.html" target="_blank">
      https://pytorch.org/vision/0.11/auto_examples/plot_visualization_utils.html</a></li>
		<li>[9] PyTorchでObeject Detection (mashykom.com)<br>
      <a href="https://www.koi.mashykom.com/pytorch2.html" target="_blank">
      https://www.koi.mashykom.com/pytorch2.html</a></li>
		<li>[10] SSDLite MobileNetV3 Backbone Object Detection with PyTorch and 
		Torchvision - DebuggerCafe<br>
		<a href="https://debuggercafe.com/ssdlite-mobilenetv3-backbone-object-detection-with-pytorch-and-torchvision/" target="_blank">
		https://debuggercafe.com/ssdlite-mobilenetv3-backbone-object-detection-with-pytorch-and-torchvision/</a></li>
		<li>[11] MobileNets: Efficient Convolutional Neural Networks for Mobile 
		Vision Applications <br>
		<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank">
		https://arxiv.org/pdf/1704.04861.pdf</a></li>
		<li>[12] MobileNetV2: Inverted Residuals and Linear Bottlenecks <br>
		<a href="https://arxiv.org/pdf/1801.04381v3.pdf" target="_blank">
		https://arxiv.org/pdf/1801.04381v3.pdf</a></li>
		<li>[13] Searching for MobileNetV3 <br>
		<a href="https://arxiv.org/pdf/1905.02244.pdf" target="_blank">
		https://arxiv.org/pdf/1905.02244.pdf</a></li>
		<li>[14] PyTorchによるSSD Mobilenetでの転移学習（Jetson Nano） | そう備忘録 
		(souichi.club)<br>
		<a href="https://www.souichi.club/deep-learning/transfer-learning/" target="_blank">
		https://www.souichi.club/deep-learning/transfer-learning/</a></li>
		<li>[15] PyTorch 新たなクラスの物体検出をSSDでやってみる | cedro-blog (cedro3.com)<br>
		<a href="http://cedro3.com/ai/pytorch-ssd-bccd/" target="_blank">
		http://cedro3.com/ai/pytorch-ssd-bccd/</a></li>
		<li>[16] MobilenetSSD : 高速に物体検出を行う機械学習モデル. ailia… | by Kazuki Kyakuno | 
		axinc | Medium<br>
		<a href="https://medium.com/axinc/mobilenetssd-高速に物体検出を行う機械学習モデル-be3ca37c411" target="_blank">
		https://medium.com/axinc/mobilenetssd-高速に物体検出を行う機械学習モデル-be3ca37c411</a></li>
		<li>[17] 画像を扱う機械学習のためのデータセットまとめ - Qiita<br>
		<a href="https://qiita.com/leetmikeal/items/7c0d23e39bf38ab8be23" target="_blank">
		https://qiita.com/leetmikeal/items/7c0d23e39bf38ab8be23</a></li>
		<li>[18]
		<a href="https://udemy.benesse.co.jp/data-science/deep-learning/transfer-learning.html#:~:text=%E8%BB%A2%E7%A7%BB%E5%AD%A6%E7%BF%92%E3%81%A8%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E9%81%95%E3%81%84%E3%81%AF%EF%BC%9F%20%E8%BB%A2%E7%A7%BB%E5%AD%A6%E7%BF%92%E3%81%A8%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AF%E3%80%81%E3%81%A9%E3%81%A1%E3%82%89%E3%82%82%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%9F%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E6%89%8B%E6%B3%95%E3%81%A7%E3%81%99%E3%80%82%20%E3%82%88%E3%81%8F%E6%B7%B7%E5%90%8C%E3%81%95%E3%82%8C%E3%81%A6%E3%81%97%E3%81%BE%E3%81%84%E3%81%BE%E3%81%99%E3%81%8C%E3%80%81%E3%81%93%E3%81%AE2%E3%81%A4%E3%81%AE%E6%89%8B%E6%B3%95%E3%81%AF%E7%95%B0%E3%81%AA%E3%82%8A%E3%81%BE%E3%81%99%E3%80%82,%E3%81%9D%E3%82%8C%E3%81%9E%E3%82%8C%E3%81%AE%E9%81%95%E3%81%84%E3%82%92%E8%A6%8B%E3%81%A6%E3%81%84%E3%81%8D%E3%81%BE%E3%81%97%E3%82%87%E3%81%86%E3%80%82%20%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AF%E3%80%81%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%B1%A4%E3%81%AE%E9%87%8D%E3%81%BF%E3%82%92%E5%BE%AE%E8%AA%BF%E6%95%B4%E3%81%99%E3%82%8B%E6%89%8B%E6%B3%95%E3%81%A7%E3%81%99%E3%80%82%20%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E9%87%8D%E3%81%BF%E3%82%92%E5%88%9D%E6%9C%9F%E5%80%A4%E3%81%A8%E3%81%97%E3%80%81%E5%86%8D%E5%BA%A6%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AB%E3%82%88%E3%81%A3%E3%81%A6%E5%BE%AE%E8%AA%BF%E6%95%B4%E3%81%97%E3%81%BE%E3%81%99%E3%80%82%20%E8%BB%A2%E7%A7%BB%E5%AD%A6%E7%BF%92%E3%81%AF%E3%80%81%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E9%87%8D%E3%81%BF%E3%81%AF%E5%9B%BA%E5%AE%9A%E3%81%97%E3%80%81%E8%BF%BD%E5%8A%A0%E3%81%97%E3%81%9F%E5%B1%A4%E3%81%AE%E3%81%BF%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%A6%E5%AD%A6%E7%BF%92%E3%81%97%E3%81%BE%E3%81%99%E3%80%82">
		転移学習とは？ディープラーニングで期待の「転移学…｜Udemy メディア (benesse.co.jp)</a></li>
	</ul>
</section>

<p>&nbsp;</p>
	<p>アノテーションツール</p>
	<ul>
		<li>heartexlabs/labelImg<br>
		<a href="https://github.com/heartexlabs/labelImg" target="_blank">
		https://github.com/heartexlabs/labelImg</a></li>
		<li>AIアノテーションツール20選を比較！タグ付け自動化ツールの選び方を紹介 (aismiley.co.jp)<br>
		<a href="https://aismiley.co.jp/ai_news/3-tools-to-perform-overlay-indispensable-for-machine-learning/" target="_blank">
		https://aismiley.co.jp/ai_news/3-tools-to-perform-overlay-indispensable-for-machine-learning/</a></li>
		<li>opencv/cvat: Annotate better with CVAT, the industry-leading data 
		engine for machine learning. Used and trusted by teams at any scale, for 
		data of any scale. (github.com)<br>
		<a href="https://github.com/opencv/cvat" target="_blank">
		https://github.com/opencv/cvat</a></li>
	</ul>
<p>&nbsp;</p>

<hr>

<p>&nbsp;</p>

<section>
	<h2 style="margin-bottom:5px">変更履歴</h2>
	<table>
	  <tr>
	    <td class="td_history_date">2023-02-06</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">新規作成 </td>
	  </tr>
	</table>
</section>

<p>&nbsp;</p>

<section>
<p><a href="../../index.html" target="_parent">Programming Items トップページ</a></p>
<p><a href="../../privacy_policy.html">プライバシーポリシー</a></p>
</section>

<p>&nbsp;</p>

<footer>
	<p><small>Copyright © 2023 Kinoshita Hidetoshi (木下英俊)</small></p>
</footer>

</body>
</html>
